#!/usr/bin/env python3
"""
Simple test for the Clarification Bot to validate its functionality.
Tests clarification question generation for different types of ambiguous queries.
"""

import asyncio
import sys
import json
from typing import Dict, Any, List

class SimpleClarificationTest:
    """Simple test for clarification bot functionality."""
    
    def __init__(self):
        self.test_results = []
        print("ğŸ§ª Initializing Clarification Bot Test")
        
        # Test scenarios for different clarification types
        self.test_scenarios = {
            "missing_government": {
                "conv_id": "test_missing_gov",
                "original_query": "×”×—×œ×˜×•×ª ×‘× ×•×©× ×—×™× ×•×š",
                "intent": "search",
                "entities": {"topic": "×—×™× ×•×š"},
                "confidence_score": 0.6,
                "clarification_type": "missing_entities"
            },
            "missing_topic": {
                "conv_id": "test_missing_topic",
                "original_query": "×”×—×œ×˜×•×ª ×××©×œ×” 37",
                "intent": "search", 
                "entities": {"government_number": 37},
                "confidence_score": 0.7,
                "clarification_type": "missing_entities"
            },
            "vague_query": {
                "conv_id": "test_vague",
                "original_query": "××” ×§×•×¨×”?",
                "intent": "search",
                "entities": {},
                "confidence_score": 0.3,
                "clarification_type": "vague_intent"
            },
            "ambiguous_time": {
                "conv_id": "test_time",
                "original_query": "×”×—×œ×˜×•×ª ×”××—×¨×•× ×•×ª",
                "intent": "search",
                "entities": {},
                "confidence_score": 0.5,
                "clarification_type": "ambiguous_time"
            },
            "low_confidence": {
                "conv_id": "test_low_conf",
                "original_query": "××•×œ×™ ××©×”×• ×¢×œ ×–×”",
                "intent": "search",
                "entities": {},
                "confidence_score": 0.2,
                "clarification_type": "low_confidence"
            }
        }
    
    def mock_generate_clarification_questions(self, scenario: Dict) -> Dict[str, Any]:
        """Mock clarification generation based on scenario type."""
        
        clarification_type = scenario["clarification_type"]
        query = scenario["original_query"]
        entities = scenario["entities"]
        
        questions = []
        
        if clarification_type == "missing_entities":
            if "government_number" not in entities:
                questions.append({
                    "type": "missing_government",
                    "question": "××™×–×” ×××©×œ×” ××ª×” ××—×¤×©?",
                    "suggestions": [
                        "×××©×œ×” 37 (× ×ª× ×™×”×• ×”× ×•×›×—×™×ª)",
                        "×××©×œ×” 36 (×‘× ×˜-×œ×¤×™×“)",
                        "×××©×œ×” 35 (× ×ª× ×™×”×• ×”×§×•×“××ª)",
                        "×›×œ ×”×××©×œ×•×ª"
                    ]
                })
            
            if "topic" not in entities and "ministries" not in entities:
                questions.append({
                    "type": "missing_topic",
                    "question": "××™×–×” × ×•×©× ××• ××©×¨×“ ××¢× ×™×™×Ÿ ××•×ª×š?",
                    "suggestions": [
                        "×—×™× ×•×š ×•×ª×¨×‘×•×ª",
                        "×‘×™×˜×—×•×Ÿ ×•×¦×‘×",
                        "×›×œ×›×œ×” ×•×ª×§×¦×™×‘",
                        "×‘×¨×™××•×ª ×•×¨×¤×•××”",
                        "××©×¨×“ ×”×—×™× ×•×š",
                        "××©×¨×“ ×”×‘×™×˜×—×•×Ÿ"
                    ]
                })
            
        elif clarification_type == "ambiguous_time":
            questions.append({
                "type": "time_clarification",
                "question": "××™×–×” ×ª×§×•×¤×ª ×–××Ÿ ××ª×” ××—×¤×©?",
                "suggestions": [
                    "×”×©× ×” ×”××—×¨×•× ×” (2023-2024)",
                    "×©× ×ª×™×™× ××—×¨×•× ×•×ª (2022-2024)",
                    "×—××© ×©× ×™× ××—×¨×•× ×•×ª (2019-2024)",
                    "×›×œ ×”×ª×§×•×¤×•×ª"
                ]
            })
            
        elif clarification_type == "vague_intent":
            questions.append({
                "type": "intent_clarification",
                "question": "××™×š ×× ×™ ×™×›×•×œ ×œ×¢×–×•×¨ ×œ×š?",
                "suggestions": [
                    "×œ×—×¤×© ×”×—×œ×˜×•×ª ×œ×¤×™ × ×•×©×",
                    "×œ××¦×•× ×”×—×œ×˜×” ×¡×¤×¦×™×¤×™×ª",
                    "×œ×¡×¤×•×¨ ×”×—×œ×˜×•×ª",
                    "×œ×¨××•×ª ×”×—×œ×˜×•×ª ××—×¨×•× ×•×ª"
                ]
            })
            
        elif clarification_type == "low_confidence":
            questions.append({
                "type": "general_clarification",
                "question": "××” ×‘×“×™×•×§ ××ª×” ××—×¤×©?",
                "suggestions": [
                    "×”×—×œ×˜×•×ª ×©×œ ×××©×œ×” ×¡×¤×¦×™×¤×™×ª",
                    "×”×—×œ×˜×•×ª ×‘× ×•×©× ××¡×•×™×",
                    "×”×—×œ×˜×” ×¡×¤×¦×™×¤×™×ª ×œ×¤×™ ××¡×¤×¨",
                    "×¡×¤×™×¨×ª ×”×—×œ×˜×•×ª"
                ]
            })
        
        # Fallback question if no specific ones generated
        if not questions:
            questions.append({
                "type": "general",
                "question": "×ª×•×›×œ ×œ×¤×¨×˜ ×™×•×ª×¨ ×›×“×™ ×©××•×›×œ ×œ×¢×–×•×¨ ×œ×š ×˜×•×‘ ×™×•×ª×¨?",
                "suggestions": [
                    "×”×•×¡×£ ××¡×¤×¨ ×××©×œ×”",
                    "×”×•×¡×£ × ×•×©× ×¡×¤×¦×™×¤×™",
                    "×”×•×¡×£ ×ª×§×•×¤×ª ×–××Ÿ",
                    "×¤×¨×˜ ××ª ××” ×©××ª×” ××—×¤×©"
                ]
            })
        
        return {
            "questions": questions,
            "explanation": f"× ×“×¨×©×™× ×¤×¨×˜×™× × ×•×¡×¤×™× ×œ×—×™×¤×•×© ××“×•×™×§ ({clarification_type})"
        }
    
    def mock_generate_suggested_refinements(self, scenario: Dict) -> List[str]:
        """Mock suggested query refinements."""
        
        query = scenario["original_query"]
        intent = scenario["intent"]
        entities = scenario["entities"]
        
        refinements = []
        
        # Add government if missing
        if "government_number" not in entities:
            refinements.extend([
                f"{query} ×××©×œ×” 37",
                f"{query} ×××©×œ×” 36",
                f"×”×—×œ×˜×•×ª ×××©×œ×” 37 {query}"
            ])
        
        # Add topic if missing
        if "topic" not in entities and "ministries" not in entities:
            if "×—×™× ×•×š" not in query.lower():
                refinements.append(f"{query} ×‘× ×•×©× ×—×™× ×•×š")
            if "×‘×™×˜×—×•×Ÿ" not in query.lower():
                refinements.append(f"{query} ×‘× ×•×©× ×‘×™×˜×—×•×Ÿ")
        
        # Add time context if missing
        if intent == "search" and "date_range" not in entities:
            refinements.extend([
                f"{query} ×‘-2023",
                f"{query} ×‘×©× ×ª×™×™× ×”××—×¨×•× ×•×ª"
            ])
        
        # Intent-specific refinements
        if intent == "search" and "×”×—×œ×˜×•×ª" not in query:
            refinements.append(f"×”×—×œ×˜×•×ª {query}")
        
        if intent == "count" and "×›××”" not in query:
            refinements.append(f"×›××” {query}")
        
        # Remove duplicates and limit
        seen = set()
        unique_refinements = []
        for ref in refinements:
            if ref not in seen and ref != query:
                seen.add(ref)
                unique_refinements.append(ref)
        
        return unique_refinements[:4]  # Limit to 4 suggestions
    
    def test_scenario(self, scenario_name: str, scenario: Dict) -> Dict[str, Any]:
        """Test a single clarification scenario."""
        
        print(f"\nğŸ“‹ Testing: {scenario_name}")
        print(f"ğŸ“¥ Query: '{scenario['original_query']}'")
        print(f"ğŸ¯ Intent: {scenario['intent']}")
        print(f"ğŸ“Š Entities: {scenario['entities']}")
        print(f"ğŸ“ˆ Confidence: {scenario['confidence_score']:.2f}")
        print(f"ğŸ” Type: {scenario['clarification_type']}")
        
        try:
            # Generate clarification questions
            clarification_data = self.mock_generate_clarification_questions(scenario)
            
            # Generate suggested refinements
            refinements = self.mock_generate_suggested_refinements(scenario)
            
            # Calculate confidence
            confidence = 0.8
            if len(clarification_data.get("questions", [])) == 1:
                confidence = 0.9
            if scenario["confidence_score"] < 0.5:
                confidence = 0.7
            
            # Create response
            response = {
                "success": True,
                "conv_id": scenario["conv_id"],
                "clarification_questions": clarification_data.get("questions", []),
                "suggested_refinements": refinements,
                "explanation": clarification_data.get("explanation", "× ×“×¨×©×™× ×¤×¨×˜×™× × ×•×¡×¤×™×"),
                "confidence": confidence
            }
            
            # Validate results
            questions_count = len(response["clarification_questions"])
            refinements_count = len(response["suggested_refinements"])
            
            print(f"    âœ… Generated {questions_count} clarification questions")
            print(f"    ğŸ”§ Generated {refinements_count} suggested refinements")
            print(f"    ğŸ“ˆ Response confidence: {response['confidence']:.2f}")
            
            # Display questions
            for i, question in enumerate(response["clarification_questions"], 1):
                print(f"    â“ Question {i}: {question['question']}")
                print(f"       ğŸ’¡ Suggestions: {len(question['suggestions'])} options")
            
            # Display refinements
            if refinements:
                print(f"    ğŸ”§ Refinement examples:")
                for ref in refinements[:2]:  # Show first 2
                    print(f"       â†’ \"{ref}\"")
            
            # Evaluation
            passed = True
            if questions_count == 0:
                print(f"    âš ï¸ No clarification questions generated")
                passed = False
            if scenario["clarification_type"] == "missing_entities" and refinements_count == 0:
                print(f"    âš ï¸ No refinements generated for missing entities")
                passed = False
            if response["confidence"] < 0.5:
                print(f"    âš ï¸ Low response confidence: {response['confidence']:.2f}")
                passed = False
            
            result = {
                "scenario_name": scenario_name,
                "passed": passed,
                "questions_count": questions_count,
                "refinements_count": refinements_count,
                "confidence": response["confidence"],
                "response": response
            }
            
            self.test_results.append(result)
            
            if passed:
                print(f"    ğŸ‰ Scenario '{scenario_name}' passed!")
            else:
                print(f"    âŒ Scenario '{scenario_name}' failed!")
            
            return result
            
        except Exception as e:
            print(f"    âŒ Error in scenario '{scenario_name}': {e}")
            result = {
                "scenario_name": scenario_name,
                "passed": False,
                "error": str(e)
            }
            self.test_results.append(result)
            return result
    
    def run_all_tests(self):
        """Run all clarification tests."""
        print("ğŸš€ Starting Clarification Bot Functionality Tests")
        print("=" * 60)
        
        # Run all test scenarios
        for scenario_name, scenario in self.test_scenarios.items():
            self.test_scenario(scenario_name, scenario)
        
        # Summary
        print("\n" + "=" * 60)
        print("ğŸ“Š Test Summary")
        
        passed = sum(1 for result in self.test_results if result.get("passed", False))
        total = len(self.test_results)
        
        print(f"âœ… Passed: {passed}/{total} scenarios")
        print(f"âŒ Failed: {total - passed}/{total} scenarios")
        
        if passed == total:
            print("ğŸ‰ All clarification tests passed!")
            print("âœ… Clarification generation is working correctly")
        else:
            print(f"âš ï¸ {total - passed} tests failed")
            
            # Show failed tests
            failed_tests = [r for r in self.test_results if not r.get("passed", False)]
            for failed in failed_tests:
                error = failed.get("error", "Unknown error")
                print(f"    âŒ {failed['scenario_name']}: {error}")
        
        # Statistics
        if self.test_results:
            questions_stats = [r.get("questions_count", 0) for r in self.test_results if "questions_count" in r]
            refinements_stats = [r.get("refinements_count", 0) for r in self.test_results if "refinements_count" in r]
            confidence_stats = [r.get("confidence", 0) for r in self.test_results if "confidence" in r]
            
            if questions_stats:
                avg_questions = sum(questions_stats) / len(questions_stats)
                print(f"\nğŸ“Š Average questions per scenario: {avg_questions:.1f}")
            
            if refinements_stats:
                avg_refinements = sum(refinements_stats) / len(refinements_stats)
                print(f"ğŸ”§ Average refinements per scenario: {avg_refinements:.1f}")
            
            if confidence_stats:
                avg_confidence = sum(confidence_stats) / len(confidence_stats)
                print(f"ğŸ“ˆ Average response confidence: {avg_confidence:.3f}")
        
        # Test specific clarification types
        print("\nğŸ” Clarification Type Analysis:")
        clarification_types = {}
        for result in self.test_results:
            if "response" in result:
                for question in result["response"].get("clarification_questions", []):
                    q_type = question.get("type", "unknown")
                    clarification_types[q_type] = clarification_types.get(q_type, 0) + 1
        
        for q_type, count in clarification_types.items():
            print(f"    ğŸ“ {q_type}: {count} questions")
        
        return passed == total

def main():
    """Main test runner."""
    test_runner = SimpleClarificationTest()
    success = test_runner.run_all_tests()
    return success

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)