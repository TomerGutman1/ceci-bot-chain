"""
EVAL Evaluator Bot 2E - Results evaluation and quality scoring.
Evaluates SQL query results for relevance, quality, and completeness.
Provides weighted scoring and explanations for ranking decisions.
"""

import json
import asyncio
import openai
import aiohttp
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime, timedelta
from enum import Enum
import re
import math

from fastapi import FastAPI, HTTPException, status
from pydantic import BaseModel, Field

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from common.logging import setup_logging, log_api_call, log_gpt_usage
from common.config import get_config

# ====================================
# CONFIGURATION & LOGGING
# ====================================
logger = setup_logging('EVAL_EVALUATOR_BOT_2E')
config = get_config('EVAL_EVALUATOR_BOT_2E')
app = FastAPI(
    title="EVAL Evaluator Bot 2E",
    description="Evaluates query results for relevance and quality",
    version="1.0.0"
)

# Configure OpenAI
openai.api_key = config.openai_api_key

# SQL Generation Bot URL
SQL_GEN_BOT_URL = os.getenv('SQL_GEN_BOT_URL', 'http://sql-gen-bot:8012')

async def fetch_decision_content(government_number: int, decision_number: int, conv_id: str) -> Optional[Dict[str, Any]]:
    """Fetch full decision content using SQL generation bot."""
    try:
        # Prepare request for SQL generation
        sql_request = {
            "intent": "search",
            "entities": {
                "government_number": government_number,
                "decision_number": decision_number
            },
            "conv_id": conv_id
        }
        
        async with aiohttp.ClientSession() as session:
            # Call SQL generation bot
            async with session.post(
                f"{SQL_GEN_BOT_URL}/sqlgen",
                json=sql_request,
                timeout=aiohttp.ClientTimeout(total=60)
            ) as response:
                if response.status != 200:
                    logger.error(f"SQL generation failed: {response.status}")
                    return None
                
                sql_result = await response.json()
                sql_query = sql_result.get("sql_query")
                parameters = sql_result.get("parameters", [])
                
                if not sql_query:
                    logger.error("No SQL query returned")
                    return None
                
                # Return None to trigger fallback to mock decision content below
                    
    except Exception as e:
        logger.error(f"Failed to fetch decision content: {e}")
        return None

# ====================================
# PYDANTIC MODELS
# ====================================
class RelevanceLevel(str, Enum):
    """Relevance levels for evaluation."""
    HIGHLY_RELEVANT = "highly_relevant"
    RELEVANT = "relevant"
    PARTIALLY_RELEVANT = "partially_relevant"
    NOT_RELEVANT = "not_relevant"

class QualityMetric(BaseModel):
    """Individual quality metric score."""
    name: str
    score: float = Field(..., ge=0.0, le=1.0)
    weight: float = Field(..., ge=0.0, le=1.0)
    explanation: str

class EvaluationRequest(BaseModel):
    """Request for feasibility evaluation of a specific government decision."""
    conv_id: str = Field(..., description="Conversation ID")
    government_number: Optional[int] = Field(default=None, description="Government number (optional)")
    decision_number: int = Field(..., description="Decision number to evaluate")
    original_query: str = Field(..., description="Original user query")

class TokenUsage(BaseModel):
    """Model for token usage tracking."""
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int
    model: str

class EvaluationResponse(BaseModel):
    """Evaluation response with scores and explanations."""
    overall_score: float = Field(..., ge=0.0, le=1.0, description="Overall quality score")
    relevance_level: RelevanceLevel = Field(..., description="Relevance classification")
    quality_metrics: List[QualityMetric] = Field(..., description="Individual quality metrics")
    content_analysis: Dict[str, Any] = Field(..., description="Content analysis results")
    recommendations: List[str] = Field(..., description="Improvement recommendations")
    confidence: float = Field(..., ge=0.0, le=1.0, description="Evaluation confidence")
    explanation: str = Field(..., description="Human-readable explanation")
    processing_time_ms: float = Field(..., description="Evaluation processing time")
    token_usage: Optional[TokenUsage] = Field(default=None, description="Token usage for GPT calls")

class HealthResponse(BaseModel):
    """Health check response."""
    status: str
    timestamp: str
    evaluation_count: int

# ====================================
# EVALUATION WEIGHTS AND THRESHOLDS
# ====================================
EVALUATION_WEIGHTS = {
    "relevance": 0.35,           # How relevant are results to query
    "completeness": 0.25,        # Are results complete/comprehensive
    "accuracy": 0.20,            # Are results accurate and up-to-date
    "entity_match": 0.15,        # Do results match extracted entities
    "performance": 0.05          # Query performance and efficiency
}

RELEVANCE_THRESHOLDS = {
    RelevanceLevel.HIGHLY_RELEVANT: 0.85,
    RelevanceLevel.RELEVANT: 0.70,
    RelevanceLevel.PARTIALLY_RELEVANT: 0.40,
    RelevanceLevel.NOT_RELEVANT: 0.0
}

# ====================================
# QUALITY METRICS EVALUATORS
# ====================================
def evaluate_relevance(query: str, intent: str, entities: Dict, results: List[Dict]) -> QualityMetric:
    """Evaluate how relevant results are to the original query."""
    if not results:
        return QualityMetric(
            name="relevance",
            score=0.0,
            weight=EVALUATION_WEIGHTS["relevance"],
            explanation="No results returned"
        )
    
    score = 0.5  # Base score
    explanation_parts = []
    
    # Check entity matching in results
    entity_matches = 0
    total_entities = len(entities)
    
    if total_entities > 0:
        for result in results[:5]:  # Check first 5 results
            for entity_key, entity_value in entities.items():
                if entity_key == "government_number" and "government_number" in result:
                    if result["government_number"] == entity_value:
                        entity_matches += 1
                        break
                elif entity_key == "topic" and "topics" in result:
                    if isinstance(result["topics"], list) and entity_value in result["topics"]:
                        entity_matches += 1
                        break
                elif entity_key == "decision_number" and "decision_number" in result:
                    if result["decision_number"] == entity_value:
                        entity_matches += 1
                        break
        
        entity_match_ratio = entity_matches / min(len(results), 5)
        score += entity_match_ratio * 0.4
        explanation_parts.append(f"{entity_matches}/{min(len(results), 5)} results match key entities")
    
    # Intent-specific relevance scoring
    if intent == "search":
        # For search, check if results have diverse topics
        if len(results) > 1:
            score += 0.1
            explanation_parts.append("Multiple results for search query")
    elif intent == "count":
        # For count, exact number matters
        score = 0.9 if len(results) == 1 else 0.6
        explanation_parts.append(f"Count query returned {len(results)} result(s)")
    elif intent == "specific_decision":
        # For specific decision, should return 1 exact match
        score = 0.95 if len(results) == 1 else 0.3
        explanation_parts.append(f"Specific decision query returned {len(results)} result(s)")
    
    # Adjust for result quality
    if results and len(results) > 0:
        # Check if results have required fields
        first_result = results[0]
        required_fields = ["title", "content", "decision_date"]
        present_fields = sum(1 for field in required_fields if field in first_result and first_result[field])
        field_score = present_fields / len(required_fields)
        score += field_score * 0.1
        explanation_parts.append(f"{present_fields}/{len(required_fields)} required fields present")
    
    score = min(1.0, score)
    explanation = "; ".join(explanation_parts) if explanation_parts else "Basic relevance evaluation"
    
    return QualityMetric(
        name="relevance",
        score=score,
        weight=EVALUATION_WEIGHTS["relevance"],
        explanation=explanation
    )

def evaluate_completeness(query: str, intent: str, entities: Dict, results: List[Dict]) -> QualityMetric:
    """Evaluate completeness of results."""
    if not results:
        return QualityMetric(
            name="completeness",
            score=0.0,
            weight=EVALUATION_WEIGHTS["completeness"],
            explanation="No results to evaluate completeness"
        )
    
    score = 0.5  # Base score
    explanation_parts = []
    
    # Check expected result count based on intent
    if intent == "search":
        if len(results) >= 10:
            score = 0.9
            explanation_parts.append("Comprehensive search results (10+ items)")
        elif len(results) >= 5:
            score = 0.8
            explanation_parts.append("Good search results (5-9 items)")
        elif len(results) >= 1:
            score = 0.6
            explanation_parts.append("Limited search results (1-4 items)")
    elif intent == "count":
        # Count should always return exactly one result
        score = 1.0 if len(results) == 1 else 0.3
        explanation_parts.append(f"Count query completeness: {len(results)} result(s)")
    elif intent == "specific_decision":
        # Specific decision should return exactly one result
        score = 1.0 if len(results) == 1 else 0.2
        explanation_parts.append(f"Specific decision completeness: {len(results)} result(s)")
    
    # Check data completeness in results
    if results:
        complete_results = 0
        for result in results[:5]:  # Check first 5
            required_fields = ["title", "content", "decision_date", "government_number"]
            present_fields = sum(1 for field in required_fields if field in result and result[field] is not None)
            if present_fields >= 3:  # At least 3 of 4 required fields
                complete_results += 1
        
        completeness_ratio = complete_results / min(len(results), 5)
        score = (score + completeness_ratio) / 2
        explanation_parts.append(f"{complete_results}/{min(len(results), 5)} results have complete data")
    
    explanation = "; ".join(explanation_parts)
    
    return QualityMetric(
        name="completeness",
        score=score,
        weight=EVALUATION_WEIGHTS["completeness"],
        explanation=explanation
    )

def evaluate_accuracy(query: str, intent: str, entities: Dict, results: List[Dict]) -> QualityMetric:
    """Evaluate accuracy of results."""
    if not results:
        return QualityMetric(
            name="accuracy",
            score=0.0,
            weight=EVALUATION_WEIGHTS["accuracy"],
            explanation="No results to evaluate accuracy"
        )
    
    score = 0.7  # Base score for returned results
    explanation_parts = []
    
    # Check date consistency
    date_consistent = True
    for result in results[:3]:  # Check first 3
        if "decision_date" in result:
            try:
                # Basic date format validation
                date_str = str(result["decision_date"])
                if len(date_str) >= 4:  # Has year
                    year = int(date_str[:4])
                    if year < 1948 or year > 2025:  # Israeli government dates
                        date_consistent = False
                        break
            except:
                date_consistent = False
                break
    
    if date_consistent:
        score += 0.1
        explanation_parts.append("Date consistency check passed")
    else:
        score -= 0.2
        explanation_parts.append("Date consistency issues detected")
    
    # Check government number consistency
    if "government_number" in entities:
        expected_gov = entities["government_number"]
        gov_consistent = True
        for result in results[:3]:
            if "government_number" in result:
                if result["government_number"] != expected_gov:
                    gov_consistent = False
                    break
        
        if gov_consistent:
            score += 0.1
            explanation_parts.append("Government number consistency verified")
        else:
            score -= 0.2
            explanation_parts.append("Government number inconsistencies found")
    
    # Check for reasonable content length
    content_quality = 0
    for result in results[:3]:
        if "content" in result and result["content"]:
            content_len = len(str(result["content"]))
            if 50 <= content_len <= 5000:  # Reasonable content length
                content_quality += 1
    
    if content_quality > 0:
        content_score = content_quality / min(len(results), 3)
        score += content_score * 0.1
        explanation_parts.append(f"{content_quality}/{min(len(results), 3)} results have quality content")
    
    score = max(0.0, min(1.0, score))
    explanation = "; ".join(explanation_parts) if explanation_parts else "Basic accuracy evaluation"
    
    return QualityMetric(
        name="accuracy",
        score=score,
        weight=EVALUATION_WEIGHTS["accuracy"],
        explanation=explanation
    )

def evaluate_entity_match(query: str, intent: str, entities: Dict, results: List[Dict]) -> QualityMetric:
    """Evaluate how well results match extracted entities."""
    if not entities:
        return QualityMetric(
            name="entity_match",
            score=1.0,  # Perfect score if no entities to match
            weight=EVALUATION_WEIGHTS["entity_match"],
            explanation="No entities to match"
        )
    
    if not results:
        return QualityMetric(
            name="entity_match",
            score=0.0,
            weight=EVALUATION_WEIGHTS["entity_match"],
            explanation="No results to match entities against"
        )
    
    total_entities = len(entities)
    matched_entities = 0
    match_details = []
    
    # Check each entity type
    for entity_key, entity_value in entities.items():
        entity_matched = False
        
        if entity_key == "government_number":
            for result in results:
                if "government_number" in result and result["government_number"] == entity_value:
                    entity_matched = True
                    break
        elif entity_key == "topic":
            for result in results:
                if "topics" in result and isinstance(result["topics"], list):
                    if entity_value in result["topics"]:
                        entity_matched = True
                        break
                elif "title" in result or "content" in result:
                    # Fuzzy topic matching in title/content
                    text_to_search = f"{result.get('title', '')} {result.get('content', '')}"
                    if entity_value in text_to_search:
                        entity_matched = True
                        break
        elif entity_key == "decision_number":
            for result in results:
                if "decision_number" in result and result["decision_number"] == entity_value:
                    entity_matched = True
                    break
        elif entity_key == "date_range":
            # Check if results fall within date range
            if isinstance(entity_value, dict) and "start" in entity_value:
                start_year = int(entity_value["start"][:4])
                for result in results:
                    if "decision_date" in result:
                        try:
                            result_year = int(str(result["decision_date"])[:4])
                            if result_year >= start_year:
                                entity_matched = True
                                break
                        except:
                            pass
        
        if entity_matched:
            matched_entities += 1
            match_details.append(f"{entity_key}:âœ“")
        else:
            match_details.append(f"{entity_key}:âœ—")
    
    score = matched_entities / total_entities if total_entities > 0 else 1.0
    explanation = f"Entity matching: {matched_entities}/{total_entities} ({', '.join(match_details)})"
    
    return QualityMetric(
        name="entity_match",
        score=score,
        weight=EVALUATION_WEIGHTS["entity_match"],
        explanation=explanation
    )

def evaluate_performance(query: str, intent: str, entities: Dict, results: List[Dict], 
                        execution_time_ms: float, result_count: int) -> QualityMetric:
    """Evaluate query performance metrics."""
    score = 0.8  # Base performance score
    explanation_parts = []
    
    # Execution time scoring
    if execution_time_ms < 100:
        score += 0.2
        explanation_parts.append("Excellent response time (<100ms)")
    elif execution_time_ms < 500:
        score += 0.1
        explanation_parts.append("Good response time (<500ms)")
    elif execution_time_ms < 1000:
        explanation_parts.append("Acceptable response time (<1s)")
    else:
        score -= 0.1
        explanation_parts.append(f"Slow response time ({execution_time_ms:.0f}ms)")
    
    # Result count efficiency
    if intent == "count":
        if result_count == 1:
            score += 0.1
            explanation_parts.append("Efficient count query (1 result)")
    elif intent == "specific_decision":
        if result_count == 1:
            score += 0.1
            explanation_parts.append("Efficient specific query (1 result)")
        elif result_count > 1:
            score -= 0.1
            explanation_parts.append(f"Inefficient specific query ({result_count} results)")
    elif intent == "search":
        if 1 <= result_count <= 20:
            score += 0.05
            explanation_parts.append("Reasonable result count for search")
        elif result_count > 50:
            score -= 0.05
            explanation_parts.append("Large result set may impact performance")
    
    score = max(0.0, min(1.0, score))
    explanation = "; ".join(explanation_parts)
    
    return QualityMetric(
        name="performance",
        score=score,
        weight=EVALUATION_WEIGHTS["performance"],
        explanation=explanation
    )

# ====================================
# DECISION SUITABILITY VALIDATION
# ====================================
def validate_decision_for_analysis(decision_content: Dict[str, Any]) -> Tuple[bool, str, str]:
    """
    Validate if a decision is suitable for feasibility analysis.
    
    Returns:
        (is_suitable, reason, suggestion)
    """
    decision_text = decision_content.get("decision_content", decision_content.get("content", ""))
    operativity = decision_content.get("operativity", "").strip()
    decision_number = decision_content.get("decision_number", "×–×•")
    
    # Check 1: Content length - less than 250 characters is too short
    if len(decision_text.strip()) < 250:
        return (
            False,
            f"×ª×•×›×Ÿ ×”×”×—×œ×˜×” ×§×¦×¨ ×ž×“×™ ×œ× ×™×ª×•×— ×ž×¢×ž×™×§ ({len(decision_text)} ×ª×•×•×™×, × ×“×¨×©×™× ×œ×¤×—×•×ª 250)",
            f"××ª×” ×™×›×•×œ ×œ×‘×§×© ××ª ×”×ª×•×›×Ÿ ×”×ž×œ× ×©×œ ×”×—×œ×˜×” {decision_number} ××• ×œ×—×¤×© ×”×—×œ×˜×•×ª ×“×•×ž×•×ª ×‘× ×•×©×."
        )
    
    # Check 2: Operativity field - use the authoritative database field
    if operativity == "×“×§×œ×¨×˜×™×‘×™×ª":
        return (
            False,
            f"×”×—×œ×˜×” ×–×• ×ž×¡×•×•×’×ª ×›'×“×§×œ×¨×˜×™×‘×™×ª' ×‘×ž×¡×“ ×”× ×ª×•× ×™× ×•×œ× ×ž×ª××™×ž×” ×œ× ×™×ª×•×— ×™×©×™×ž×•×ª",
            "× ×™×ª×•×— ×™×©×™×ž×•×ª ×ž×™×•×¢×“ ×œ×”×—×œ×˜×•×ª ××•×¤×¨×˜×™×‘×™×•×ª ×”×›×•×œ×œ×•×ª ×™×™×©×•×, ×”×§×¦××ª ×ž×©××‘×™× ×•×œ×•×—×•×ª ×–×ž× ×™×. ××ª×” ×™×›×•×œ ×œ×—×¤×© ×”×—×œ×˜×•×ª ××•×¤×¨×˜×™×‘×™×•×ª ×‘× ×•×©× ×“×•×ž×” ××• ×œ×‘×§×© ×¡×™×›×•× ×©×œ ×”×”×—×œ×˜×”."
        )
    
    # Check 3: Missing operativity field - this shouldn't happen with real data
    if not operativity:
        logger.warning(f"Decision {decision_number} missing operativity field")
        # If operativity is missing, we don't automatically reject - let it through for analysis
        # But log it for investigation
    
    return (True, "", "")

# ====================================
# GPT-POWERED CONTENT ANALYSIS
# ====================================
async def perform_feasibility_analysis(decision_content: Dict[str, Any], request: EvaluationRequest) -> EvaluationResponse:
    """Perform comprehensive feasibility analysis of a government decision."""
    start_time = datetime.utcnow()
    
    # Pre-analysis validation: Check if decision is suitable for analysis
    is_suitable, reason, suggestion = validate_decision_for_analysis(decision_content)
    
    if not is_suitable:
        # Return informative response instead of performing expensive analysis
        logger.info(f"Decision {decision_content.get('decision_number')} not suitable for analysis: {reason}")
        
        processing_time = (datetime.utcnow() - start_time).total_seconds() * 1000
        
        informative_message = f"""âš ï¸ **×œ× × ×™×ª×Ÿ ×œ×‘×¦×¢ × ×™×ª×•×— ×™×©×™×ž×•×ª ×œ×”×—×œ×˜×” ×–×•**

ðŸ“ **×¡×™×‘×”:** {reason}

ðŸ’¡ **×”×¦×¢×•×ª ×—×œ×•×¤×™×•×ª:** {suggestion}

â„¹ï¸ **×”×¡×‘×¨:** × ×™×ª×•×— ×™×©×™×ž×•×ª ×ž×™×•×¢×“ ×œ×”×—×œ×˜×•×ª ×ž×“×™× ×™×•×ª ×ž×•×¨×›×‘×•×ª ×”×›×•×œ×œ×•×ª ×™×™×©×•×, ×”×§×¦××ª ×ž×©××‘×™× ×•×œ×•×—×•×ª ×–×ž× ×™×. ×”×—×œ×˜×•×ª ×“×§×œ×¨×˜×™×‘×™×•×ª ××• ×§×¦×¨×•×ª ××™× ×Ÿ ×ž×ª××™×ž×•×ª ×œ× ×™×ª×•×— ×–×”."""

        # Return a structured response that looks like a regular evaluation but explains why analysis wasn't performed
        return EvaluationResponse(
            overall_score=0.0,
            relevance_level=RelevanceLevel.NOT_RELEVANT,
            quality_metrics=[],
            content_analysis={
                "analysis_status": "not_suitable",
                "reason": reason,
                "suggestion": suggestion,
                "decision_title": decision_content.get("decision_title", decision_content.get("title", "×œ×œ× ×›×•×ª×¨×ª")),
                "content_length": len(decision_content.get("decision_content", decision_content.get("content", ""))),
                "informative_message": informative_message
            },
            recommendations=[f"×”×—×œ×˜×” ×–×• ××™× ×” ×ž×ª××™×ž×” ×œ× ×™×ª×•×— ×™×©×™×ž×•×ª: {reason}"],
            confidence=1.0,  # We're very confident this decision isn't suitable
            explanation=informative_message,
            processing_time_ms=processing_time,
            token_usage=TokenUsage(
                prompt_tokens=0,
                completion_tokens=0,
                total_tokens=0,
                model="validation-only"
            )
        )
    
    # Extract decision details
    decision_title = decision_content.get("decision_title", "×œ×œ× ×›×•×ª×¨×ª")
    decision_text = decision_content.get("decision_content", decision_content.get("content", ""))
    summary = decision_content.get("summary", "")
    
    # Prepare the complete decision text for analysis
    full_decision_text = f"""
×›×•×ª×¨×ª ×”×”×—×œ×˜×”: {decision_title}

×ª×•×›×Ÿ ×”×”×—×œ×˜×”:
{decision_text}

×ª×§×¦×™×¨:
{summary}
"""
    
    # Create the analysis prompt with the full decision content and detailed criteria
    prompt = f"""× ×ª×— ××ª ×”×—×œ×˜×ª ×”×ž×ž×©×œ×” ×”×‘××” ×œ×¤×™ 13 ×”×§×¨×™×˜×¨×™×•× ×™× ×œ× ×™×ª×•×— ×™×©×™×ž×•×ª:

{full_decision_text}

×‘×¦×¢ × ×™×ª×•×— ×™×©×™×ž×•×ª ×ž×¤×•×¨×˜ ×œ×¤×™ ×”×§×¨×™×˜×¨×™×•× ×™× ×”×‘××™×. ×¢×œ ×›×œ ×§×¨×™×˜×¨×™×•×Ÿ ×ª×Ÿ ×¦×™×•×Ÿ ×ž-0 ×¢×“ 5 ×œ×¤×™ ×”×”× ×—×™×•×ª ×”×ž×¤×•×¨×˜×•×ª:

**1. ×œ×•×— ×–×ž× ×™× ×ž×—×™×™×‘ (×ž×©×§×œ 17%)**
×‘×“×•×§ ×”×× ×‘×”×”×—×œ×˜×” ×ž×•×’×“×¨×™× ×ª××¨×™×›×™× ××• ×“×“-×œ×™×™× ×™× ×ž×—×™×™×‘×™× ×•×ž×” ×§×•×¨×” ×× ×œ× ×¢×•×ž×“×™× ×‘×–×ž× ×™×:
- 0: ××™×Ÿ ××–×›×•×¨ ×–×ž×Ÿ ×‘×™×¦×•×¢
- 1: ××ž×™×¨×” ×›×œ×œ×™×ª ×›×ž×• "×‘×”×§×“×" 
- 2: ××–×›×•×¨ ×ª××¨×™×š ××—×“ ×œ×¡×¢×™×£ ×©×•×œ×™
- 3: ×–×ž× ×™× ×œ×¨×•×‘ ×”×¡×¢×™×¤×™× ××š ×œ× ×‘×¨×•×¨ ×× ×ž×—×™×™×‘
- 4: ×ª××¨×™×›×™× ×‘×¨×•×¨×™× ×œ×›×œ ×ž×©×™×ž×” ×¢×™×§×¨×™×ª
- 5: ×ª××¨×™×›×™× ×ž×—×™×™×‘×™× ×œ×›×œ ×¡×¢×™×£ ×›×•×œ×œ ×”×’×“×¨×ª ××™-×¢×ž×™×“×”

**2. ×¦×•×•×ª ×ž×ª×›×œ×œ (×ž×©×§×œ 7%)**
×’×•×¨× ×©×ž×ª×× ×‘×™×Ÿ ×›×œ×œ ×”×’×•×¨×ž×™× ×•×ž×•×•×“× ×‘×™×¦×•×¢:
- 0: ××™×Ÿ ×¦×•×•×ª ×ž×ª×›×œ×œ
- 1: ××–×›×•×¨ ×ž×¢×•×¨×¤×œ ×œ×¦×•×•×ª ×¢×ª×™×“×™
- 2: ×ž×•×–×›×¨ ×¦×•×•×ª ××š ×œ×œ× ×¤×™×¨×•×˜
- 3: ×’×•×£ ×ž×•×’×“×¨ ×œ×ª×›×œ×•×œ ××š ×¡×ž×›×•×™×•×ª×™×• ×œ× ×‘×¨×•×¨×•×ª
- 4: ×¦×•×•×ª ×ž×•×’×“×¨ ×”×™×˜×‘ ×¢× ××—×¨×™×•×ª ×›×•×œ×œ×ª
- 5: ×ª×™××•×¨ ×ž×œ× ×©×œ ×”×¦×•×•×ª, ×—×‘×¨×™×, ×¡×ž×›×•×™×•×ª ×•×ª×“×™×¨×•×ª ×ž×¤×’×©×™×

**3. ×’×•×¨× ×ž×ª×›×œ×œ ×™×—×™×“ (×ž×©×§×œ 5%)**
××“× ×¡×¤×¦×™×¤×™ ×©××—×¨××™ ×¢×œ ×›×œ×œ ×”×ž×”×œ×š:
- 0: ××™×Ÿ ××“× ×™×—×™×“ ×ž×•×–×›×¨
- 1: ×©×¨ ××—×¨××™ ×‘×¨×ž×ª ×›×•×ª×¨×ª ×‘×œ×‘×“
- 2: ×ž×ž×•× ×” ×™×—×™×“ ××š ×œ× ×‘×¨×•×¨ ×ª×¤×§×™×“×•
- 3: ×‘×¢×œ ×ª×¤×§×™×“ ×‘×¨×•×¨ ××š ×œ× ×¢×œ ×›×œ ×”×’×•×¤×™×
- 4: ×¨××© ×¤×¨×•×™×§×˜ ×¢× ××—×¨×™×•×ª ×œ×ª×›×œ×•×œ
- 5: ××“× ×ž×•×’×“×¨ ×¨×©×ž×™×ª ×¢× ×¤×™×¨×•×˜ ×ž×œ× ×©×œ ×”×¡×ž×›×•×ª

**4. ×ž× ×’× ×•×Ÿ ×“×™×•×•×—/×‘×§×¨×” (×ž×©×§×œ 9%)**
×œ×ž×™ ×ž×“×•×•×—×™×, ×‘××™×–×• ×ª×“×™×¨×•×ª ×•×‘××™×–×• ×ž×ª×›×•× ×ª:
- 0: ××™×Ÿ ××–×›×•×¨ ×œ×“×™×•×•×—
- 1: ××ž×™×¨×” ×›×œ×œ×™×ª ×¢×œ ×¢×“×›×•× ×™×
- 2: ×ž×¦×•×™×Ÿ ×’×•×£ ×œ×“×™×•×•×— ××š ×œ× ×ª×“×™×¨×•×ª
- 3: ×ž× ×’× ×•×Ÿ ×“×™×•×•×— ×¡×‘×™×¨ ××š ×œ× ×‘×¨×•×¨ ×”×ž×‘× ×”
- 4: ×ž× ×’× ×•×Ÿ ×ž×¤×•×¨×˜ ××š ×œ× ×‘×¨×•×¨ ×ž×” ×¢×•×©×™× ×¢× ×”×“×™×•×•×—
- 5: ×ž× ×’× ×•×Ÿ ×ž×•×‘× ×” ×¢× ×ª×“×™×¨×•×ª, ×¤×•×¨×ž×˜ ×•×ª×’×•×‘×” ×œ×—×¨×™×’×•×ª

**5. ×ž× ×’× ×•×Ÿ ×ž×“×™×“×” ×•×”×¢×¨×›×” (×ž×©×§×œ 6%)**
×›×™×¦×“ ×œ×ž×“×•×“ ××¤×§×˜×™×‘×™×•×ª ×”×”×—×œ×˜×” ×‘×¤×•×¢×œ:
- 0: ××™×Ÿ ××–×›×•×¨ ×œ×ž×“×™×“×”
- 1: ××ž×™×¨×” ×›×œ×œ×™×ª ×¢×œ ×‘×—×™× ×ª ×”×©×¤×¢×”
- 2: ×›×•×•× ×” ×œ×‘×—×•×Ÿ ×‘××ž×¦×¢×•×ª ×ž×—×§×¨ ××š ×‘×œ×™ ×ª×›× ×™×ª
- 3: ×ª×•×›× ×™×ª ×‘×¡×™×¡×™×ª ×œ×ž×“×™×“×” ××š ×—×¡×¨×™× ×¤×¨×˜×™×
- 4: ×ž× ×’× ×•×Ÿ ×ž×¡×•×“×¨ ××š ×—×¡×¨×™× ×¤×¨×˜×™× ×˜×›× ×™×™×
- 5: ×ž×ª×•×•×” ×ž×œ× ×¢× ×ž×“×“×™×, ×œ×•×—×•×ª ×–×ž× ×™× ×•×’×•×£ ×ž×•×¡×ž×š

**6. ×ž× ×’× ×•×Ÿ ×‘×™×§×•×¨×ª ×—×™×¦×•× ×™×ª (×ž×©×§×œ 4%)**
×’×•×¨× ×—×™×¦×•× ×™ ×œ×‘×“×™×§×ª ×”×ª×”×œ×™×š ×•×”×¢×ž×™×“×” ×‘×™×¢×“×™×:
- 0: ××™×Ÿ ×’×•×£ ×—×™×¦×•× ×™
- 1: ×©×™×§×•×œ ×œ×‘×§×© ×ž×ž×•×ž×—×™× ×—×™×¦×•× ×™×™×
- 2: ×ž×¦×•×™×Ÿ ×’×•×£ ×—×™×¦×•× ×™ ×‘×œ×™ ×¤×™×¨×•×˜ ×ª×¤×§×™×“×•
- 3: ×’×•×£ ×‘×™×§×•×¨×ª ×ž×•×’×“×¨ ××š ×œ× ×‘×¨×•×¨ ××™×š ×•×ž×ª×™
- 4: ×’×•×£ ×—×™×¦×•× ×™ ×¢× ×ž×•×¢×“ ×œ×”×¢×¨×›×”
- 5: ×‘×™×§×•×¨×ª ×—×™×¦×•× ×™×ª ×ž×¤×•×¨×˜×ª ×¢× ×ª×“×™×¨×•×ª ×•×™×™×©×•× ×”×ž×œ×¦×•×ª

**7. ×ž×©××‘×™× × ×“×¨×©×™× (×ž×©×§×œ 19%)**
×ª×§×¦×™×‘, ×›×•×— ××“× ×•×ž×§×•×¨×•×ª ×ž×™×ž×•×Ÿ ×œ×™×™×©×•×:
- 0: ××™×Ÿ ××–×›×•×¨ ×œ×ž×©××‘×™×
- 1: ××ž×™×¨×” ×›×œ×œ×™×ª ×¢×œ ×¦×•×¨×š ×‘×ª×§×¦×™×‘
- 2: ××–×›×•×¨ ×¡×›×•× ××• ×ž×§×•×¨ ××š ×œ× ×‘×¨×•×¨ ×”×—×œ×•×§×”
- 3: ×¡×›×•× ×ž×¦×•×™×Ÿ ××š ×œ× ×‘×¨×•×¨ ×”×—×œ×•×§×” ××• ×”×ž×§×•×¨
- 4: ×¤×™×¨×•×˜ ×”×ž×§×•×¨ ×•×©×™×ž×•×© ×¢×™×§×¨×™, ×—×¡×¨ ×›×•×— ××“×
- 5: ×¦×™×•×Ÿ ×ž×¤×•×¨×˜ ×©×œ ×¡×›×•×ž×™×, ×—×œ×•×§×”, ×›×•×— ××“× ×•×ž×” ×¢×•×©×™× ×‘×¢×™×›×•×‘×™×

**8. ×ž×¢×•×¨×‘×•×ª ×©×œ ×ž×¡×¤×¨ ×“×¨×’×™× ×‘×ª×”×œ×™×š (×ž×©×§×œ 7%)**
×©×™×œ×•×‘ ×“×¨×’×™× ×©×•× ×™×: ×©×¨, ×ž× ×›"×œ×™×, ×’×•×¨×ž×™× ×ž×§×¦×•×¢×™×™× ×•×›×•':
- 0: ×¨×§ ×“×¨×’ ×™×—×™×“
- 1: ×’×•×£ × ×•×¡×£ ×œ×”×ª×™×™×¢×¦×•×ª ××š ×œ× ×‘×¨×•×¨ ×ª×¤×§×™×“×•
- 2: ×¨×©×™×ž×ª ×“×¨×’×™× ××š ×œ× ×ž×•×¡×‘×¨ ×©×™×ª×•×£ ×”×¤×¢×•×œ×”
- 3: ×¤×™×¨×•×˜ ×¢×§×¨×•× ×™ ××š ×œ× ×‘×¨×•×¨ ×ž× ×’× ×•×Ÿ ×”×ž×¤×’×©×™×
- 4: ×ž×¢×•×¨×‘×•×ª ×ž×•×’×“×¨×ª ×™×¤×”, ×—×¡×¨ ×¤×™×¨×•×˜ ×”××™× ×˜×¨××§×¦×™×”
- 5: ×ª×™××•×¨ ×ž×œ× ×¢× ×œ×•×—×•×ª ×–×ž× ×™× ×•× ×”×œ×™ ×ª×™××•×

**9. ×ž×‘× ×” ×¡×¢×™×¤×™× ×•×—×œ×•×§×ª ×¢×‘×•×“×” ×‘×¨×•×¨×” (×ž×©×§×œ 9%)**
×—×œ×•×§×” ×œ×¡×¢×™×¤×™× ×‘×¨×•×¨×™× ×¢× ×”×’×“×¨×ª ××—×¨×™×•×ª:
- 0: ×˜×§×¡×˜ ×ž×‘×•×œ×’×Ÿ ×œ×œ× ×¡×¢×™×¤×™×
- 1: ×¡×¢×™×¤×™× ×¢×ž×•×ž×™× ×œ×œ× ×‘×”×™×¨×•×ª ××—×¨×™×•×ª
- 2: ×›×ž×” ×¡×¢×™×¤×™× ×¢× ××—×¨×™×•×ª ××š ×¨×•×‘× ×œ× ×‘×¨×•×¨×™×
- 3: ×‘××•×¤×Ÿ ×›×œ×œ×™ ××—×¨×™×•×ª ×ž×•×’×“×¨×ª ××š ×ž×©×™×ž×•×ª ×›×œ×œ×™×•×ª
- 4: ×¡×¢×™×¤×™× ×ž×¡×•×“×¨×™× ×¢× ××—×¨×™×•×ª ××š ×—×¡×¨×” ×”×’×“×¨×ª ×©×œ×‘×™×
- 5: ×¡×¢×™×¤×™× ×‘×¨×•×¨×™× ×¢× ×’×•×£ ××—×¨××™ ×•×ž×©×™×ž×” ×ž×¤×•×¨×˜×ª

**10. ×ž× ×’× ×•×Ÿ ×™×™×©×•× ×‘×©×˜×— (×ž×©×§×œ 9%)**
×›×™×¦×“ ×‘×¤×•×¢×œ ×™×‘×•×¦×¢ ×”×™×™×©×•× ×•×ž×™ ×™×‘×¦×¢:
- 0: ××™×Ÿ ××–×›×•×¨ ×œ××™×š ×ž×™×™×©×ž×™×
- 1: ××ž×™×¨×” ×§×¦×¨×” ×¢×œ ×™×™×©×•× ×“×¨×š ×¨×©×•×™×•×ª
- 2: ×ž× ×’× ×•×Ÿ ×›×œ×œ×™ ×œ×œ× ×”×¡×‘×¨ ×¡×ž×›×•×™×•×ª
- 3: ×ž× ×’× ×•×Ÿ ×‘×¡×™×¡×™ ××š ×œ× ×‘×¨×•×¨ ××™×š ×™×¢×‘×“×•
- 4: ×ž× ×’× ×•×Ÿ ×§×•× ×§×¨×˜×™, ×—×¡×¨×™× ×¤×¨×˜×™× ×˜×›× ×™×™×
- 5: ×ª×™××•×¨ ×©×œ× ×©×œ ×”×‘×™×¦×•×¢, ×¡×ž×›×•×™×•×ª ×•×¤×™×§×•×—

**11. ×’×•×¨× ×ž×›×¨×™×¢ (×ž×©×§×œ 3%)**
×ž×™ ×ž×›×¨×™×¢ ×‘×ž×—×œ×•×§×•×ª ×‘×™×Ÿ ×’×•×¤×™× ×ž×¢×•×¨×‘×™×:
- 0: ××™×Ÿ ×ž× ×’× ×•×Ÿ ×”×›×¨×¢×”
- 1: ××ž×™×¨×” ×›×œ×œ×™×ª ×¢×œ ×¡×ž×›×•×ª ×”×©×¨
- 2: ××–×›×•×¨ ×’×•×¨× ×ž×›×¨×™×¢ ××š ×œ× ×‘×¨×•×¨ ×ª×”×œ×™×š
- 3: ×’×•×¨× ×ž×›×¨×™×¢ ×ž×•×–×›×¨ ××š ××™×Ÿ ×¤×™×¨×•×˜ ×“×™×•×Ÿ
- 4: ×’×•×¨× ×ž×›×¨×™×¢ ×‘×¨×•×¨ ××š ××™×Ÿ ×”×’×“×¨×” ×ž×” ×‘×•×“×§
- 5: ×’×•×¨× ×ž×›×¨×™×¢ ×ž×¤×•×¨×˜ ×¢× ×ª×”×œ×™×š ×”×›×¨×¢×”

**12. ×©×•×ª×¤×•×ª ×‘×™×Ÿ ×ž×’×–×¨×™×ª (×ž×©×§×œ 3%)**
×ž×¢×•×¨×‘×•×ª ×ž×’×–×¨×™× × ×•×¡×¤×™× ×ž×¢×‘×¨ ×œ×ž×ž×©×œ×”:
- 0: ××™×Ÿ ×’×•×¨× ×—×•×¥-×ž×ž×©×œ×ª×™
- 1: ××ž×™×¨×” ×¡×ª×ž×™×ª ×¢×œ ×©×™×ª×•×£ ×ž×’×–×¨×™×
- 2: ××¨×’×•×Ÿ ×ž×¡×•×™× ×ž×•×–×›×¨ ××š ×œ× ×ª×¤×§×™×“×•
- 3: ×ž×’×–×¨ ×©×•×ª×£ ×œ×ª×”×œ×™×š ××š ×œ× ×‘×¨×•×¨ ×”×ž× ×“×˜
- 4: ×¤×™×¨×•×˜ ××¨×’×•× ×™× ×•××•×¤×Ÿ ×¢×‘×•×“×”, ×—×¡×¨×™× ×’×‘×•×œ×•×ª ××—×¨×™×•×ª
- 5: ×©×™×ª×•×£ ×ž×¤×•×¨×˜ ×¢× ×¨×©×™×ž×”, ×ž×˜×¨×” ×•×ž× ×’× ×•×Ÿ ×”×ª×§×©×¨×•×ª

**13. ×ž×“×“×™ ×ª×•×¦××” ×•×ž×¨×›×™×‘×™ ×”×¦×œ×—×” (×ž×©×§×œ 2%)**
×”×’×“×¨×ª ×™×¢×“×™× ×›×ž×•×ª×™×™× ××• ××™×›×•×ª×™×™× ×‘×¨×•×¨×™×:
- 0: ××™×Ÿ ×”×’×“×¨×” ×œ×™×¢×“ ×ª×•×¦××ª×™
- 1: ××ž×™×¨×” ×¢×ž×•×ž×” ×¢×œ ×©×™×¤×•×¨
- 2: ×™×¢×“ ×›×œ×œ×™ ××š ×œ× ×ž×•×’×“×¨ ×›×ž×” ××• ×ž×ª×™
- 3: ×™×¢×“ ×ž×¡×¤×¨×™ ××š ××™×Ÿ ×ª××¨×™×š ××• ×©×™×˜×ª ×ž×“×™×“×”
- 4: ×™×¢×“ ×›×ž×•×ª×™ ×•×ž×¡×’×¨×ª ×–×ž×Ÿ ××š ×œ× ×‘×¨×•×¨ ××™×š ×ž×•×“×“×™×
- 5: ×™×¢×“×™× ×ž×¡×¤×¨×™×™× ×‘×¨×•×¨×™× ×¢× ×ž×ª×•×“×•×œ×•×’×™×” ×•×ª×’×•×‘×” ×œ××™-×¢×ž×™×“×”

×”×—×–×¨ ×ª×•×¦××” ×‘×¤×•×¨×ž×˜ JSON ×”×ž×“×•×™×§ ×”×–×”:
{{"criteria": [{{"name": "×œ×•×— ×–×ž× ×™× ×ž×—×™×™×‘", "score": 0, "explanation": "×”×¡×‘×¨ ×§×¦×¨", "weight": 17}}, {{"name": "×¦×•×•×ª ×ž×ª×›×œ×œ", "score": 0, "explanation": "×”×¡×‘×¨ ×§×¦×¨", "weight": 7}}, {{"name": "×’×•×¨× ×ž×ª×›×œ×œ ×™×—×™×“", "score": 0, "explanation": "×”×¡×‘×¨ ×§×¦×¨", "weight": 5}}, {{"name": "×ž× ×’× ×•×Ÿ ×“×™×•×•×—/×‘×§×¨×”", "score": 0, "explanation": "×”×¡×‘×¨ ×§×¦×¨", "weight": 9}}, {{"name": "×ž× ×’× ×•×Ÿ ×ž×“×™×“×” ×•×”×¢×¨×›×”", "score": 0, "explanation": "×”×¡×‘×¨ ×§×¦×¨", "weight": 6}}, {{"name": "×ž× ×’× ×•×Ÿ ×‘×™×§×•×¨×ª ×—×™×¦×•× ×™×ª", "score": 0, "explanation": "×”×¡×‘×¨ ×§×¦×¨", "weight": 4}}, {{"name": "×ž×©××‘×™× × ×“×¨×©×™×", "score": 0, "explanation": "×”×¡×‘×¨ ×§×¦×¨", "weight": 19}}, {{"name": "×ž×¢×•×¨×‘×•×ª ×©×œ ×ž×¡×¤×¨ ×“×¨×’×™× ×‘×ª×”×œ×™×š", "score": 0, "explanation": "×”×¡×‘×¨ ×§×¦×¨", "weight": 7}}, {{"name": "×ž×‘× ×” ×¡×¢×™×¤×™× ×•×—×œ×•×§×ª ×¢×‘×•×“×” ×‘×¨×•×¨×”", "score": 0, "explanation": "×”×¡×‘×¨ ×§×¦×¨", "weight": 9}}, {{"name": "×ž× ×’× ×•×Ÿ ×™×™×©×•× ×‘×©×˜×—", "score": 0, "explanation": "×”×¡×‘×¨ ×§×¦×¨", "weight": 9}}, {{"name": "×’×•×¨× ×ž×›×¨×™×¢", "score": 0, "explanation": "×”×¡×‘×¨ ×§×¦×¨", "weight": 3}}, {{"name": "×©×•×ª×¤×•×ª ×‘×™×Ÿ ×ž×’×–×¨×™×ª", "score": 0, "explanation": "×”×¡×‘×¨ ×§×¦×¨", "weight": 3}}, {{"name": "×ž×“×“×™ ×ª×•×¦××” ×•×ž×¨×›×™×‘×™ ×”×¦×œ×—×”", "score": 0, "explanation": "×”×¡×‘×¨ ×§×¦×¨", "weight": 2}}], "weighted_score": 0.0, "final_score": 0, "summary": "×¡×™×›×•× ×”× ×™×ª×•×—", "decision_title": "×›×•×ª×¨×ª ×”×”×—×œ×˜×”", "decision_number": 0, "government_number": 0}}

×—×©×•×‘ ×ž××•×“ ×œ×—×™×©×•×‘ final_score:
1. ×›×œ ×§×¨×™×˜×¨×™×•×Ÿ ×ž×§×‘×œ ×¦×™×•×Ÿ 0-5
2. ×”×¦×™×•×Ÿ ×”×ž×©×•×§×œ×œ ×œ×›×œ ×§×¨×™×˜×¨×™×•×Ÿ = (×¦×™×•×Ÿ ×”×§×¨×™×˜×¨×™×•×Ÿ / 5) * ×ž×©×§×œ ×”×§×¨×™×˜×¨×™×•×Ÿ
3. final_score = ×¡×›×•× ×›×œ ×”×¦×™×•× ×™× ×”×ž×©×•×§×œ×œ×™× (×™×”×™×” ×‘×™×Ÿ 0-100)
4. ×“×•×’×ž×”: ×× ×§×¨×™×˜×¨×™×•×Ÿ ×¢× ×ž×©×§×œ 17% ×§×™×‘×œ ×¦×™×•×Ÿ 5, ×ª×¨×•×ž×ª×• ×”×™× (5/5)*17 = 17

×—×©×•×‘: ×”×—×–×¨ ×¨×§ JSON ×ª×§×™×Ÿ, ×œ×œ× ×˜×§×¡×˜ × ×•×¡×£ ×œ×¤× ×™ ××• ××—×¨×™.
"""
    
    try:
        # Call GPT for feasibility analysis
        response = await asyncio.to_thread(
            openai.ChatCompletion.create,
            model="gpt-4-turbo",  # Always use GPT-4 for evaluator
            messages=[
                {"role": "system", "content": "××ª×” ×ž× ×ª×— ×ž×•×ž×—×” ×œ×™×©×™×ž×•×ª ×”×—×œ×˜×•×ª ×ž×ž×©×œ×”. ×”×ª×¤×§×™×“ ×©×œ×š ×œ× ×ª×— ×”×—×œ×˜×•×ª ×œ×¤×™ 13 ×§×¨×™×˜×¨×™×•× ×™× ×ž×•×’×“×¨×™× ×•×œ×”×—×–×™×¨ ×ª×•×¦××” ×‘×¤×•×¨×ž×˜ JSON ×ž×“×•×™×§. ×¢×œ×™×š ×œ×§×¨×•× ×‘×–×”×™×¨×•×ª ××ª ×ª×•×›×Ÿ ×”×”×—×œ×˜×” ×•×œ×”×¢×¨×™×š ×›×œ ×§×¨×™×˜×¨×™×•×Ÿ ×œ×¤×™ ×”×¡×§××œ×” ×”×ž×•×’×“×¨×ª (0-5). ×—×©×‘ ××ª final_score ×›×š: ×¡×›×•× ×©×œ [(×¦×™×•×Ÿ ×›×œ ×§×¨×™×˜×¨×™×•×Ÿ / 5) * ×ž×©×§×œ ×”×§×¨×™×˜×¨×™×•×Ÿ] ×¢×‘×•×¨ ×›×œ 13 ×”×§×¨×™×˜×¨×™×•× ×™×. ×”×ª×•×¦××” ×¦×¨×™×›×” ×œ×”×™×•×ª ×‘×™×Ÿ 0-100. ×”×—×–×¨ ×¨×§ JSON ×ª×§×™×Ÿ ×œ×œ× ×˜×§×¡×˜ × ×•×¡×£. ××œ ×ª×©×ª×ž×© ×‘×¢×™×¦×•×‘ Bold ××• ×¡×™×ž× ×™× ×ž×™×•×—×“×™× ×‘×ª×•×š ×”-JSON."},
                {"role": "user", "content": prompt}
            ],
            temperature=config.temperature,
            max_tokens=4000
        )
        
        # Extract token usage
        usage = response.usage
        token_usage = TokenUsage(
            prompt_tokens=usage.prompt_tokens,
            completion_tokens=usage.completion_tokens,
            total_tokens=usage.total_tokens,
            model="gpt-4-turbo"  # Always use GPT-4 for evaluator
        )
        
        # Log GPT usage
        log_gpt_usage(
            logger,
            model="gpt-4-turbo",  # Always use GPT-4 for evaluator
            prompt_tokens=usage.prompt_tokens,
            completion_tokens=usage.completion_tokens,
            total_tokens=usage.total_tokens
        )
        
        # Parse GPT response
        content = response.choices[0].message.content.strip()
        
        try:
            # Extract JSON from response - try multiple approaches
            analysis_result = None
            
            # First try: look for JSON block
            json_start = content.find('{')
            json_end = content.rfind('}') + 1
            if json_start >= 0 and json_end > json_start:
                json_str = content[json_start:json_end]
                try:
                    analysis_result = json.loads(json_str)
                except json.JSONDecodeError:
                    # Try to fix common JSON issues
                    # Remove any trailing text after the last }
                    lines = json_str.split('\n')
                    clean_lines = []
                    brace_count = 0
                    for line in lines:
                        for char in line:
                            if char == '{':
                                brace_count += 1
                            elif char == '}':
                                brace_count -= 1
                        clean_lines.append(line)
                        if brace_count == 0:
                            break
                    
                    cleaned_json = '\n'.join(clean_lines)
                    # Fix common issues like trailing commas
                    cleaned_json = cleaned_json.replace(',}', '}').replace(',]', ']')
                    analysis_result = json.loads(cleaned_json)
            
            if analysis_result:
                
                # Extract analysis data
                criteria = analysis_result.get("criteria", [])
                final_score = analysis_result.get("final_score", 0)
                weighted_score = analysis_result.get("weighted_score", final_score)
                summary = analysis_result.get("summary", "× ×™×ª×•×— ×”×•×©×œ×")
                decision_title = analysis_result.get("decision_title", decision_title)
                decision_num = analysis_result.get("decision_number", request.decision_number)
                gov_num = analysis_result.get("government_number", request.government_number)
                
                # Convert to expected format
                quality_metrics = []
                for criterion in criteria:
                    quality_metrics.append(QualityMetric(
                        name=criterion.get("name", "×§×¨×™×˜×¨×™×•×Ÿ"),
                        score=float(criterion.get("score", 0)) / 5.0,  # Convert 0-5 to 0-1
                        weight=float(criterion.get("weight", 1)) / 100.0,  # Convert percentage to decimal
                        explanation=criterion.get("explanation", "")
                    ))
                
                # Calculate overall score
                overall_score = float(final_score) / 100.0  # Convert 0-100 to 0-1
                
                # Determine relevance level and category based on score  
                if overall_score >= 0.75:
                    relevance_level = RelevanceLevel.HIGHLY_RELEVANT
                    feasibility_category = "×™×©×™×ž×•×ª ×’×‘×•×”×”"
                elif overall_score >= 0.50:
                    relevance_level = RelevanceLevel.RELEVANT
                    feasibility_category = "×™×©×™×ž×•×ª ×‘×™× ×•× ×™×ª"
                else:
                    relevance_level = RelevanceLevel.PARTIALLY_RELEVANT
                    feasibility_category = "×™×©×™×ž×•×ª × ×ž×•×›×”"
                
                # Generate formatted analysis table
                criteria_table = []
                for criterion in criteria:
                    criteria_table.append(f"| {criterion.get('name', '×§×¨×™×˜×¨×™×•×Ÿ')} | {criterion.get('weight', 0)}% | {criterion.get('score', 0)} | {criterion.get('explanation', '')} |")
                
                table_header = "| ×§×¨×™×˜×¨×™×•×Ÿ | ×ž×©×§×œ | ×¦×™×•×Ÿ (0â€“5) | × ×™×ž×•×§ |\n|---|---|---|---|"
                criteria_table_str = table_header + "\n" + "\n".join(criteria_table)
                
                # Create detailed explanation in the required format
                formatted_explanation = f"""ðŸ” × ×™×ª×•×— ×”×—×œ×˜×ª ×ž×ž×©×œ×” {decision_num} ×œ×¤×™ ×§×¨×™×˜×¨×™×•× ×™ ×”×™×™×©×•×

**×›×•×ª×¨×ª ×”×”×—×œ×˜×”:** {decision_title}

{criteria_table_str}

ðŸ§® **×—×™×©×•×‘ ×¦×™×•×Ÿ ×™×©×™×ž×•×ª ×ž×©×•×§×œ×œ**
×”×¦×™×•×Ÿ ×”×›×•×œ×œ ×©×œ ×”×—×œ×˜×ª ×ž×ž×©×œ×” {decision_num} ×”×•× {final_score}%, ×›×œ×•×ž×¨:
âœ… ×¨×ž×ª ×™×©×™×ž×•×ª: {feasibility_category}

ðŸ“ **×¡×™×›×•× × ×™×ª×•×— ×•××‘×—× ×•×ª ×¢×™×§×¨×™×•×ª**
{summary}

ðŸ”§ **×”×ž×œ×¦×•×ª ×œ×©×™×¤×•×¨**
×‘×”×ª×‘×¡×¡ ×¢×œ ×”× ×™×ª×•×—, × ×™×ª×Ÿ ×œ×©×¤×¨ ××ª ×¨×ž×ª ×”×™×©×™×ž×•×ª ×¢×œ ×™×“×™ ×”×ª×ž×§×“×•×ª ×‘×§×¨×™×˜×¨×™×•× ×™× ×©×§×™×‘×œ×• ×¦×™×•×Ÿ × ×ž×•×š."""
                
                processing_time = (datetime.utcnow() - start_time).total_seconds() * 1000
                
                return EvaluationResponse(
                    overall_score=overall_score,
                    relevance_level=relevance_level,
                    quality_metrics=quality_metrics,
                    content_analysis={"feasibility_analysis": summary, "decision_title": decision_title, "criteria_breakdown": criteria},
                    recommendations=[f"×¦×™×•×Ÿ ×™×©×™×ž×•×ª ×›×•×œ×œ: {final_score}/100"],
                    confidence=0.9,
                    explanation=formatted_explanation,
                    processing_time_ms=processing_time,
                    token_usage=token_usage
                )
                
            else:
                # Fallback if no JSON found
                fallback_token_usage = TokenUsage(
                    prompt_tokens=0,
                    completion_tokens=0,
                    total_tokens=0,
                    model="fallback"
                )
                return EvaluationResponse(
                    overall_score=0.5,
                    relevance_level=RelevanceLevel.PARTIALLY_RELEVANT,
                    quality_metrics=[],
                    content_analysis={"error": "×œ× × ×™×ª×Ÿ ×œ×¤×¨×¡×¨ ××ª ×ª×•×¦××•×ª ×”× ×™×ª×•×—"},
                    recommendations=["× ×“×¨×© × ×™×ª×•×— ×ž×—×“×©"],
                    confidence=0.3,
                    explanation=f"× ×™×ª×•×— ×™×©×™×ž×•×ª × ×›×©×œ - ×ª×’×•×‘×ª GPT: {content[:200]}...",
                    processing_time_ms=(datetime.utcnow() - start_time).total_seconds() * 1000,
                    token_usage=fallback_token_usage
                )
                
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse feasibility analysis JSON: {e}")
            fallback_token_usage = TokenUsage(
                prompt_tokens=0,
                completion_tokens=0,
                total_tokens=0,
                model="fallback"
            )
            return EvaluationResponse(
                overall_score=0.5,
                relevance_level=RelevanceLevel.PARTIALLY_RELEVANT,
                quality_metrics=[],
                content_analysis={"error": "×©×’×™××” ×‘×¤×™×¢× ×•×— ×ª×•×¦××•×ª ×”× ×™×ª×•×—"},
                recommendations=["× ×“×¨×© × ×™×ª×•×— ×ž×—×“×©"],
                confidence=0.3,
                explanation=f"× ×™×ª×•×— ×™×©×™×ž×•×ª × ×›×©×œ - ×©×’×™××ª JSON: {str(e)}",
                processing_time_ms=(datetime.utcnow() - start_time).total_seconds() * 1000,
                token_usage=fallback_token_usage
            )
            
    except Exception as e:
        logger.error(f"Feasibility analysis failed: {e}")
        fallback_token_usage = TokenUsage(
            prompt_tokens=0,
            completion_tokens=0,
            total_tokens=0,
            model="fallback"
        )
        return EvaluationResponse(
            overall_score=0.0,
            relevance_level=RelevanceLevel.NOT_RELEVANT,
            quality_metrics=[],
            content_analysis={"error": f"× ×™×ª×•×— × ×›×©×œ: {str(e)}"},
            recommendations=["×‘×“×•×§ ××ª ×”×—×œ×˜×” ×•× ×¡×” ×©×•×‘"],
            confidence=0.1,
            explanation=f"× ×™×ª×•×— ×™×©×™×ž×•×ª × ×›×©×œ ×¢× ×©×’×™××”: {str(e)}",
            processing_time_ms=(datetime.utcnow() - start_time).total_seconds() * 1000,
            token_usage=fallback_token_usage
        )

async def analyze_content_with_gpt(query: str, intent: str, entities: Dict, results: List[Dict]) -> Tuple[Dict[str, Any], Optional[TokenUsage]]:
    """Use GPT to analyze content quality and relevance."""
    if not results:
        no_results_token_usage = TokenUsage(
            prompt_tokens=0,
            completion_tokens=0,
            total_tokens=0,
            model="no-results"
        )
        return {
            "semantic_relevance": 0.0,
            "content_quality": 0.0,
            "language_quality": 0.0,
            "gpt_explanation": "No results to analyze"
        }, no_results_token_usage
    
    # Prepare sample results for GPT analysis (limit to 3 for efficiency)
    sample_results = results[:3]
    results_text = ""
    
    for i, result in enumerate(sample_results, 1):
        title = result.get("title", "No title")
        content = result.get("content", "No content")
        # Truncate content for GPT
        content_snippet = content[:300] + "..." if len(content) > 300 else content
        results_text += f"Result {i}:\nTitle: {title}\nContent: {content_snippet}\n\n"
    
    prompt = f"""× ×ª×— ××ª ×”×—×œ×˜×ª ×”×ž×ž×©×œ×” ×”×‘××” ×œ×¤×™ 13 ×”×§×¨×™×˜×¨×™×•× ×™×:

×”×—×œ×˜×” ×œ× ×™×ª×•×—:
×›×•×ª×¨×ª: {results_text}

×‘×¦×¢ × ×™×ª×•×— ×™×©×™×ž×•×ª ×©×œ ×”×—×œ×˜×ª ×ž×ž×©×œ×” ×‘×”×ª×× ×œ×§×¨×™×˜×¨×™×•× ×™× ×”×‘××™×. ×¢×œ ×›×œ ×¤×¨×ž×˜×¨ ×™×© ×œ×”×§×¦×•×ª × ×™×§×•×“ ×ž-0 ×¢×“ 5.

×“×¨×’ ×›×œ ×§×¨×™×˜×¨×™×•×Ÿ ×•×ª×Ÿ ×”×¡×‘×¨ ×§×¦×¨:
1. ×œ×•×— ×–×ž× ×™× ×ž×—×™×™×‘ (×ž×©×§×œ 17%)
2. ×¦×•×•×ª ×ž×ª×›×œ×œ (×ž×©×§×œ 7%) 
3. ×’×•×¨× ×ž×ª×›×œ×œ ×™×—×™×“ (×ž×©×§×œ 5%)
4. ×ž× ×’× ×•×Ÿ ×“×™×•×•×—/×‘×§×¨×” (×ž×©×§×œ 9%)
5. ×ž× ×’× ×•×Ÿ ×ž×“×™×“×” ×•×”×¢×¨×›×” (×ž×©×§×œ 6%)
6. ×ž× ×’× ×•×Ÿ ×‘×™×§×•×¨×ª ×—×™×¦×•× ×™×ª (×ž×©×§×œ 4%)
7. ×ž×©××‘×™× × ×“×¨×©×™× (×ž×©×§×œ 19%)
8. ×ž×¢×•×¨×‘×•×ª ×©×œ ×ž×¡×¤×¨ ×“×¨×’×™× ×‘×ª×”×œ×™×š (×ž×©×§×œ 7%)
9. ×ž×‘× ×” ×¡×¢×™×¤×™× ×•×—×œ×•×§×ª ×¢×‘×•×“×” ×‘×¨×•×¨×” (×ž×©×§×œ 9%)
10. ×ž× ×’× ×•×Ÿ ×™×™×©×•× ×‘×©×˜×— (×ž×©×§×œ 9%)
11. ×’×•×¨× ×ž×›×¨×™×¢ (×ž×©×§×œ 3%)
12. ×©×•×ª×¤×•×ª ×‘×™×Ÿ ×ž×’×–×¨×™×ª (×ž×©×§×œ 3%)
13. ×ž×“×“×™ ×ª×•×¦××” ×•×ž×¨×›×™×‘×™ ×”×¦×œ×—×” (×ž×©×§×œ 2%)

×”×—×–×¨ ×ª×•×¦××” ×‘×¤×•×¨×ž×˜ JSON: {{"criteria": [{{"name": "×©× ×”×§×¨×™×˜×¨×™×•×Ÿ", "score": 3, "explanation": "×”×¡×‘×¨ ×§×¦×¨"}}], "final_score": 65, "summary": "×¡×™×›×•× ×›×œ×œ×™"}}
"""
    
    try:
        start_time = datetime.utcnow()
        
        response = await asyncio.to_thread(
            openai.ChatCompletion.create,
            model="gpt-4-turbo",  # Always use GPT-4 for evaluator
            messages=[
                {"role": "system", "content": "×‘×¦×¢ × ×™×ª×•×— ×™×©×™×ž×•×ª ×©×œ ×”×—×œ×˜×ª ×ž×ž×©×œ×” ×‘×”×ª×× ×œ×§×¨×™×˜×¨×™×•× ×™× ×©×”×•×’×“×¨×• ×‘×›×œ ××—×“ ×ž×”×¤×¨×ž×˜×¨×™× ×”×‘××™×. ×¢×œ ×›×œ ×¤×¨×ž×˜×¨ ×™×© ×œ×”×§×¦×•×ª × ×™×§×•×“ ×ž-0 ×¢×“ 5 ×œ×¤×™ ×”×ž×“×¨×’×•×ª ×©× ×§×‘×¢×•. ×œ××—×¨ ×ž×›×Ÿ ×ª×¦×™×’ ××ª ×”×¦×™×•×Ÿ ×”×¡×•×¤×™ ×œ×¤×™ ×”×ž×©×§×œ×™× ×‘×¡×•×£. ×›×œ ×¤×¨×ž×˜×¨ ×ž×”×¤×¨×ž×˜×¨×™× ×”× \"×œ ×™×§×‘×œ ×¢×¨×š ×ž×¡×¤×¨×™ ×‘×™×Ÿ 0 ×œ-5 ×¢×œ ×¤×™ ×”×”× ×—×™×•×ª ×•× ×™×ž×•×§ ×§×¦×¨ ×œ×ª×•×¦××”. ×™×© ×œ×¡×›× ××ª ×”× ×™×ª×•×— ×‘×¦×•×¨×” ×‘×¨×•×¨×” ×•×ž×ž×•×§×“×ª ×›×•×œ×œ ×”×¦×™×•×Ÿ ×œ×›×œ ×§×¨×™×˜×¨×™×•×Ÿ. ×ª×¦×™×’ ××ª ×”×§×¨×™×˜×¨×™×•× ×™× ×•×”×¡×‘×¨ ×¢×œ ×”×¦×™×•×Ÿ. ×ª×¨××” ××ª ×”×—×™×©×•×‘ ×©×œ ×”×¦×™×•×Ÿ ×”×¡×•×¤×™ ×œ×¤×™ ×§×¨×™×˜×¨×™×•× ×™× ×•×ª×ž×™×¨ ××•×ª×• ×œ×¦×™×•×Ÿ ×‘×™×Ÿ 0 ×œ 100. Don't write in bold. Instead of \"\\times\" use \"*\". Don't use \"\\\""},
                {"role": "user", "content": prompt}
            ],
            temperature=config.temperature,
            max_tokens=4000
        )
        
        duration = (datetime.utcnow() - start_time).total_seconds()
        
        # Extract token usage
        usage = response.usage
        token_usage = TokenUsage(
            prompt_tokens=usage.prompt_tokens,
            completion_tokens=usage.completion_tokens,
            total_tokens=usage.total_tokens,
            model="gpt-4-turbo"  # Always use GPT-4 for evaluator
        )
        
        # Log GPT usage
        log_gpt_usage(
            logger,
            model="gpt-4-turbo",  # Always use GPT-4 for evaluator
            prompt_tokens=usage.prompt_tokens,
            completion_tokens=usage.completion_tokens,
            total_tokens=usage.total_tokens
        )
        
        # Parse GPT response
        gpt_text = response.choices[0].message.content.strip()
        
        # Try to extract JSON from response with improved parsing
        try:
            # Look for JSON in the response
            gpt_analysis = None
            json_start = gpt_text.find('{')
            json_end = gpt_text.rfind('}') + 1
            if json_start >= 0 and json_end > json_start:
                json_str = gpt_text[json_start:json_end]
                try:
                    gpt_analysis = json.loads(json_str)
                except json.JSONDecodeError:
                    # Try to clean up the JSON
                    lines = json_str.split('\n')
                    clean_lines = []
                    brace_count = 0
                    for line in lines:
                        for char in line:
                            if char == '{':
                                brace_count += 1
                            elif char == '}':
                                brace_count -= 1
                        clean_lines.append(line)
                        if brace_count == 0:
                            break
                    
                    cleaned_json = '\n'.join(clean_lines)
                    cleaned_json = cleaned_json.replace(',}', '}').replace(',]', ']')
                    gpt_analysis = json.loads(cleaned_json)
            
            if gpt_analysis:
                
                return {
                    "semantic_relevance": float(gpt_analysis.get("semantic_relevance", 0.5)),
                    "content_quality": float(gpt_analysis.get("content_quality", 0.5)),
                    "language_quality": float(gpt_analysis.get("language_quality", 0.5)),
                    "gpt_explanation": gpt_analysis.get("explanation", "GPT analysis completed")
                }, token_usage
            else:
                # Fallback if no JSON found
                return {
                    "semantic_relevance": 0.6,
                    "content_quality": 0.6,
                    "language_quality": 0.6,
                    "gpt_explanation": f"GPT response: {gpt_text[:100]}..."
                }, token_usage
                
        except json.JSONDecodeError:
            # Fallback parsing
            return {
                "semantic_relevance": 0.6,
                "content_quality": 0.6,
                "language_quality": 0.6,
                "gpt_explanation": f"Parse error: {gpt_text[:100]}..."
            }, token_usage
            
    except Exception as e:
        logger.error(f"GPT content analysis failed: {e}")
        fallback_token_usage = TokenUsage(
            prompt_tokens=0,
            completion_tokens=0,
            total_tokens=0,
            model="fallback"
        )
        return {
            "semantic_relevance": 0.5,
            "content_quality": 0.5,
            "language_quality": 0.5,
            "gpt_explanation": f"Analysis failed: {str(e)}"
        }, fallback_token_usage

# ====================================
# MAIN EVALUATION LOGIC
# ====================================
async def evaluate_results(request: EvaluationRequest) -> EvaluationResponse:
    """Main evaluation logic combining all metrics."""
    start_time = datetime.utcnow()
    
    # Calculate individual quality metrics
    relevance_metric = evaluate_relevance(
        request.original_query, request.intent, request.entities, request.results
    )
    
    completeness_metric = evaluate_completeness(
        request.original_query, request.intent, request.entities, request.results
    )
    
    accuracy_metric = evaluate_accuracy(
        request.original_query, request.intent, request.entities, request.results
    )
    
    entity_match_metric = evaluate_entity_match(
        request.original_query, request.intent, request.entities, request.results
    )
    
    performance_metric = evaluate_performance(
        request.original_query, request.intent, request.entities, request.results,
        request.execution_time_ms, request.result_count
    )
    
    quality_metrics = [
        relevance_metric, completeness_metric, accuracy_metric,
        entity_match_metric, performance_metric
    ]
    
    # Calculate weighted overall score
    overall_score = sum(metric.score * metric.weight for metric in quality_metrics)
    
    # Determine relevance level
    relevance_level = RelevanceLevel.NOT_RELEVANT
    for level, threshold in sorted(RELEVANCE_THRESHOLDS.items(), key=lambda x: x[1], reverse=True):
        if overall_score >= threshold:
            relevance_level = level
            break
    
    # GPT-powered content analysis
    content_analysis, token_usage = await analyze_content_with_gpt(
        request.original_query, request.intent, request.entities, request.results
    )
    
    # Generate recommendations
    recommendations = []
    
    if relevance_metric.score < 0.7:
        recommendations.append("×©×™×¤×•×¨ ×¨×œ×•×•× ×˜×™×•×ª: ×©×§×•×œ ×—×™×“×•×“ ×”×©××™×œ×ª× ××• ×”×ª××ž×ª ××œ×’×•×¨×™×ª× ×”×—×™×¤×•×©")
    
    if completeness_metric.score < 0.6:
        recommendations.append("×©×™×¤×•×¨ ×©×œ×ž×•×ª: ×”×¨×—×‘ ××ª ×”×™×§×£ ×”×—×™×¤×•×© ××• ×©×¤×¨ ×ž×™×•×Ÿ ×”×ª×•×¦××•×ª")
    
    if accuracy_metric.score < 0.7:
        recommendations.append("×©×™×¤×•×¨ ×“×™×•×§: ×‘×“×•×§ ××ª ××™×›×•×ª ×”× ×ª×•× ×™× ×•×ª×”×œ×™×›×™ ×”×•×œ×™×“×¦×™×”")
    
    if entity_match_metric.score < 0.8:
        recommendations.append("×©×™×¤×•×¨ ×”×ª××ž×ª ×™×©×•×™×•×ª: ×©×¤×¨ ×–×™×”×•×™ ×™×©×•×™×•×ª ××• ×ž×™×¤×•×™ ×ž×¡×“ ×”× ×ª×•× ×™×")
    
    if performance_metric.score < 0.7:
        recommendations.append("×©×™×¤×•×¨ ×‘×™×¦×•×¢×™×: ××•×¤×˜× ×©××™×œ×ª×•×ª SQL ××• ×©×¤×¨ ××™× ×“×§×¡×™×")
    
    if not recommendations:
        recommendations.append("×”×ª×•×¦××•×ª ×‘××™×›×•×ª ×˜×•×‘×” - ×”×ž×©×š ×œ×©×ž×•×¨ ×¢×œ ×”×¨×ž×”")
    
    # Evaluation confidence based on result count and consistency
    confidence = 0.8  # Base confidence
    
    if len(request.results) >= 3:
        confidence += 0.1  # More confident with more results
    elif len(request.results) == 0:
        confidence -= 0.2  # Less confident with no results
    
    if overall_score > 0.8 or overall_score < 0.3:
        confidence += 0.1  # More confident in clear good/bad cases
    
    confidence = max(0.1, min(0.95, confidence))
    
    # Generate human-readable explanation
    explanation = f"""
×”×¢×¨×›×ª ××™×›×•×ª ×”×ª×•×¦××•×ª:
- ×¦×™×•×Ÿ ×›×œ×œ×™: {overall_score:.2f}/1.00 ({relevance_level.value})
- ×¨×œ×•×•× ×˜×™×•×ª: {relevance_metric.score:.2f} ({relevance_metric.explanation})
- ×©×œ×ž×•×ª: {completeness_metric.score:.2f} ({completeness_metric.explanation})
- ×“×™×•×§: {accuracy_metric.score:.2f} ({accuracy_metric.explanation})
- ×”×ª××ž×ª ×™×©×•×™×•×ª: {entity_match_metric.score:.2f} ({entity_match_metric.explanation})
- ×‘×™×¦×•×¢×™×: {performance_metric.score:.2f} ({performance_metric.explanation})

× ×™×ª×•×— ×ª×•×›×Ÿ GPT: {content_analysis.get('gpt_explanation', '×œ× ×–×ž×™×Ÿ')}
""".strip()
    
    processing_time = (datetime.utcnow() - start_time).total_seconds() * 1000
    
    return EvaluationResponse(
        overall_score=overall_score,
        relevance_level=relevance_level,
        quality_metrics=quality_metrics,
        content_analysis=content_analysis,
        recommendations=recommendations,
        confidence=confidence,
        explanation=explanation,
        processing_time_ms=processing_time,
        token_usage=token_usage
    )

# ====================================
# API ENDPOINTS
# ====================================
evaluation_count = 0

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Health check endpoint."""
    return HealthResponse(
        status="ok",
        timestamp=datetime.utcnow().isoformat(),
        evaluation_count=evaluation_count
    )

@app.post("/evaluate", response_model=EvaluationResponse)
async def evaluate_decision_feasibility(request: EvaluationRequest):
    """Main feasibility evaluation endpoint for government decisions."""
    global evaluation_count
    start_time = datetime.utcnow()
    
    try:
        log_api_call(
            logger, "evaluate_decision_feasibility", request.model_dump(),
            request.conv_id, "EVAL_EVALUATOR_BOT_2E"
        )
        
        # Create test scenarios for edge case validation
        decision_number = request.decision_number
        
        # Create different mock decisions to test edge cases
        if decision_number == 1111:  # Test short content
            decision_content = {
                "id": f"test-{decision_number}",
                "government_number": request.government_number or 37,
                "decision_number": decision_number,
                "decision_title": "×”×—×œ×˜×” ×§×¦×¨×” ×œ×‘×“×™×§×”",
                "decision_content": "×–×•×”×™ ×”×—×œ×˜×” ×§×¦×¨×” ×ž××•×“ ×¢× ×¤×—×•×ª ×ž-250 ×ª×•×•×™× ×œ×‘×“×™×§×ª ×”×œ×•×’×™×§×” ×”×—×“×©×”.",  # <250 chars
                "content": "×–×•×”×™ ×”×—×œ×˜×” ×§×¦×¨×” ×ž××•×“ ×¢× ×¤×—×•×ª ×ž-250 ×ª×•×•×™× ×œ×‘×“×™×§×ª ×”×œ×•×’×™×§×” ×”×—×“×©×”.",
                "summary": "×”×—×œ×˜×” ×§×¦×¨×”",
                "decision_date": "2024-01-15",
                "operativity": "××•×¤×¨×˜×™×‘×™×ª",  # Adding DB field
                "topics": ["×‘×“×™×§×”"],
                "ministries": ["×ž×©×¨×“ ×”×‘×“×™×§×•×ª"]
            }
        elif decision_number == 2222:  # Test declarative decision
            decision_content = {
                "id": f"test-{decision_number}",
                "government_number": request.government_number or 37,
                "decision_number": decision_number,
                "decision_title": "×”×¦×“×¢×” ×œ×–×›×¨×• ×©×œ ×”××œ×ž×•× ×™",
                "decision_content": "×”×ž×ž×©×œ×” ×ž×—×œ×™×˜×” ×œ×”×‘×™×¢ ×”×¦×“×¢×” ×œ×–×›×¨×• ×©×œ ×”××œ×ž×•× ×™ ×¢×œ ×ª×¨×•×ž×ª×• ×”×ž×©×ž×¢×•×ª×™×ª ×œ×—×‘×¨×” ×”×™×©×¨××œ×™×ª ×•×œ×ž×“×™× ×ª ×™×©×¨××œ. ×”×ž×ž×©×œ×” ×ž×‘×§×©×ª ×œ×”×›×™×¨ ×‘×¤×•×¢×œ×• ×”×—×©×•×‘ ×•×œ×—×œ×•×§ ×›×‘×•×“ ×œ×ž×©×¤×—×ª×• ×•×œ×§×¨×•×‘×™×•. ×”×ž×ž×©×œ×” ×ž×‘×§×©×ª ×œ×”×•×“×•×ª ×œ×• ×¢×œ ×ž×¡×™×¨×•×ª×•, ×ž×—×•×™×‘×•×ª×• ×•×ª×¨×•×ž×ª×• ×”×™×™×—×•×“×™×ª. ×–×•×”×™ ×”×—×œ×˜×” ×“×§×œ×¨×˜×™×‘×™×ª ×©××™× ×” ×ž×ª××™×ž×” ×œ× ×™×ª×•×— ×™×©×™×ž×•×ª ×ž×“×™× ×™×•×ª ×›×™×•×•×Ÿ ×©×”×™× ××™× ×” ×›×•×œ×œ×ª ×™×™×©×•×, ×ª×§×¦×™×‘ ××• ×œ×•×—×•×ª ×–×ž× ×™× ×ž×¢×©×™×™×, ××œ× ×ž×”×•×•×” ×”×‘×¢×ª ×”×•×§×¨×” ×•×›×‘×•×“ ×‘×œ×‘×“. ×”×—×œ×˜×•×ª ×ž×¡×•×’ ×–×” × ×•×¢×“×• ×œ×”×‘×™×¢ ×¢×ž×“×•×ª ×¢×¨×›×™×•×ª ×•×œ× ×œ×™×™×©×•× ×ž×“×™× ×™×•×ª ×ž×ž×©×™×ª.",
                "content": "×”×ž×ž×©×œ×” ×ž×—×œ×™×˜×” ×œ×”×‘×™×¢ ×”×¦×“×¢×” ×œ×–×›×¨×• ×©×œ ×”××œ×ž×•× ×™ ×¢×œ ×ª×¨×•×ž×ª×• ×œ×—×‘×¨×” ×”×™×©×¨××œ×™×ª.",
                "summary": "×”×¦×“×¢×” ×œ×–×›×¨×• ×©×œ ×”××œ×ž×•× ×™",
                "decision_date": "2024-01-15",
                "operativity": "×“×§×œ×¨×˜×™×‘×™×ª",  # Adding DB field - this will trigger rejection
                "topics": ["×”×•×§×¨×”"],
                "ministries": ["×ž×©×¨×“ ×¨××© ×”×ž×ž×©×œ×”"]
            }
        else:  # Regular decision for analysis
            decision_content = {
                "id": f"decision-{decision_number}",
                "government_number": request.government_number or 37,
                "decision_number": decision_number,
                "decision_title": f"×”×—×œ×˜×ª ×ž×ž×©×œ×” ×ž×¡×¤×¨ {decision_number}",
                "decision_content": f"×–×•×”×™ ×”×—×œ×˜×ª ×ž×ž×©×œ×” ×ž×¡×¤×¨ {decision_number} ×œ×¦×•×¨×š ×‘×“×™×§×ª ×ž×¢×¨×›×ª ×”× ×™×ª×•×—. ×”×”×—×œ×˜×” ×›×•×œ×œ×ª ×”×§×¦××ª ×ª×§×¦×™×‘ ×©×œ 50 ×ž×™×œ×™×•×Ÿ ×©×§×œ, ×”×’×“×¨×ª ×œ×•×— ×–×ž× ×™× ×ž×¤×•×¨×˜ ×¢× ××‘× ×™ ×“×¨×š ×¨×‘×¢×•× ×™×•×ª, ×•×ž×ª×Ÿ ××—×¨×™×•×ª ×‘×¨×•×¨×” ×œ×’×•×¨×ž×™× ×¨×œ×•×•× ×˜×™×™× ×‘×ž×©×¨×“×™ ×”×ž×ž×©×œ×”. ×™×© ×¦×•×¨×š ×‘×™×™×©×•× ×ž×œ× ×‘×ª×•×š 18 ×—×•×“×©×™× ×¢× ×“×™×•×•×— ×—×•×“×©×™ ×¢×œ ×”×”×ª×§×“×ž×•×ª ×œ×ž×©×¨×“ ×¨××© ×”×ž×ž×©×œ×”. ×”×”×—×œ×˜×” ×›×•×œ×œ×ª ×’× ×ž× ×’× ×•×Ÿ ×‘×§×¨×” ×—×™×¦×•× ×™ ×¢×œ ×™×“×™ ×ž×©×¨×“ ×”××•×¦×¨ ×•×ž×¢×•×¨×‘×•×ª ×©×œ ××¨×’×•× ×™× ×¦×™×‘×•×¨×™×™× ×‘×‘×™×¦×•×¢. ×”×ž×˜×¨×” ×”×™× ×œ×”×§×™× ×ª×•×›× ×™×ª ×¨×—×‘×ª ×”×™×§×£ ×œ×©×™×¤×•×¨ ×”×©×™×¨×•×ª×™× ×”×¦×™×‘×•×¨×™×™×, ×›×•×œ×œ ×”×›×©×¨×ª ×¢×•×‘×“×™×, ×©×“×¨×•×’ ×ž×¢×¨×›×•×ª ×ž×™×“×¢ ×•×™×¦×™×¨×ª ×ž× ×’× ×•× ×™ ×ž×©×•×‘ ×ž×”×¦×™×‘×•×¨.",
                "content": f"×–×•×”×™ ×”×—×œ×˜×ª ×ž×ž×©×œ×” ×ž×¡×¤×¨ {decision_number} ×œ×¦×•×¨×š ×‘×“×™×§×ª ×ž×¢×¨×›×ª ×”× ×™×ª×•×—.",
                "summary": f"×”×—×œ×˜×” {decision_number} - ×”×§×¦××ª ×ª×§×¦×™×‘, ×™×™×©×•× ×ž×ª×•×›× ×Ÿ ×•×‘×§×¨×” ×—×™×¦×•× ×™×ª",
                "decision_date": "2024-01-15",
                "operativity": "××•×¤×¨×˜×™×‘×™×ª",  # Adding DB field - this allows analysis
                "topics": ["×ª×§×¦×™×‘", "×™×™×©×•×", "×“×™×•×•×—", "×‘×§×¨×”"],
                "ministries": ["×ž×©×¨×“ ×”××•×¦×¨", "×ž×©×¨×“ ×¨××© ×”×ž×ž×©×œ×”"]
            }
        
        logger.info(f"Using mock decision content for analysis of decision {decision_number} (length: {len(decision_content['decision_content'])} chars)")
        
        # Perform feasibility analysis
        evaluation = await perform_feasibility_analysis(decision_content, request)
        
        evaluation_count += 1
        
        # Log successful evaluation
        duration = (datetime.utcnow() - start_time).total_seconds()
        logger.info(
            "Feasibility evaluation completed",
            extra={
                "conv_id": request.conv_id,
                "government_number": request.government_number,
                "decision_number": request.decision_number,
                "overall_score": evaluation.overall_score,
                "duration_ms": duration * 1000
            }
        )
        
        return evaluation
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(
            f"Feasibility evaluation failed for conv_id {request.conv_id}: {e}",
            extra={"conv_id": request.conv_id, "error": str(e)},
            exc_info=True
        )
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Evaluation failed: {str(e)}"
        )

@app.get("/metrics")
async def get_evaluation_metrics():
    """Get evaluation metrics and statistics."""
    return {
        "total_evaluations": evaluation_count,
        "evaluation_weights": EVALUATION_WEIGHTS,
        "relevance_thresholds": {k.value: v for k, v in RELEVANCE_THRESHOLDS.items()},
        "supported_intents": ["search", "count", "specific_decision", "comparison"],
        "quality_metrics": ["relevance", "completeness", "accuracy", "entity_match", "performance"]
    }

# ====================================
# STARTUP
# ====================================
if __name__ == "__main__":
    import uvicorn
    port = config.port if hasattr(config, 'port') else 8014
    uvicorn.run(app, host="0.0.0.0", port=port)