#!/usr/bin/env python3
"""
Simple test for the Formatter Bot to validate its functionality.
Tests different output formats and presentation styles for search results.
"""

import json
import sys
from typing import Dict, Any, List

class SimpleFormatterTest:
    """Simple test for formatter bot functionality."""
    
    def __init__(self):
        self.test_results = []
        print("ğŸ§ª Initializing Formatter Bot Test")
        
        # Sample formatted results for testing
        self.sample_results = [
            {
                "id": 1,
                "government_number": 37,
                "decision_number": 660,
                "decision_date": "2023-05-15",
                "title": "×”×—×œ×˜×” ×‘× ×•×©× ×—×™× ×•×š ×“×™×’×™×˜×œ×™",
                "content": "×”×—×œ×˜×” ××¤×•×¨×˜×ª ×¢×œ ×§×™×“×•× ×”×—×™× ×•×š ×”×“×™×’×™×˜×œ×™ ×‘××¢×¨×›×ª ×”×—×™× ×•×š ×”×™×©×¨××œ×™×ª. ×›×•×œ×œ ×ª×§×¦×™×‘ ×©×œ 500 ××™×œ×™×•×Ÿ ×©×§×œ ×œ×©×œ×•×© ×©× ×™×.",
                "topics": ["×—×™× ×•×š", "×˜×›× ×•×œ×•×’×™×”"],
                "ministries": ["××©×¨×“ ×”×—×™× ×•×š"],
                "status": "approved",
                "_ranking": {
                    "total_score": 0.95,
                    "explanation": "×“×™×¨×•×’ ×”×™×‘×¨×™×“×™"
                }
            },
            {
                "id": 2,
                "government_number": 37,
                "decision_number": 661,
                "decision_date": "2023-06-20",
                "title": "×ª×§×¦×™×‘ ××©×¨×“ ×”×—×™× ×•×š ×œ×©× ×ª 2024",
                "content": "××™×©×•×¨ ×ª×§×¦×™×‘ ××©×¨×“ ×”×—×™× ×•×š ×œ×©× ×ª ×”×›×¡×¤×™× 2024 ×‘×¡×š 65 ××™×œ×™××¨×“ ×©×§×œ.",
                "topics": ["×—×™× ×•×š", "×ª×§×¦×™×‘"],
                "ministries": ["××©×¨×“ ×”×—×™× ×•×š"],
                "status": "approved",
                "_ranking": {
                    "total_score": 0.85,
                    "explanation": "×“×™×¨×•×’ ×œ×¤×™ ×¨×œ×•×•× ×˜×™×•×ª"
                }
            }
        ]
        
        self.sample_evaluation = {
            "overall_score": 0.87,
            "relevance_level": "highly_relevant",
            "confidence": 0.92
        }
        
        # Test scenarios for different formatting options
        self.test_scenarios = {
            "markdown_detailed": {
                "conv_id": "test_markdown",
                "original_query": "×”×—×œ×˜×•×ª ×‘× ×•×©× ×—×™× ×•×š ×“×™×’×™×˜×œ×™",
                "intent": "search",
                "entities": {"topic": "×—×™× ×•×š"},
                "output_format": "markdown",
                "presentation_style": "detailed",
                "include_metadata": True,
                "include_scores": True
            },
            "json_compact": {
                "conv_id": "test_json",
                "original_query": "×”×—×œ×˜×•×ª ×××©×œ×” 37",
                "intent": "search",
                "entities": {"government_number": 37},
                "output_format": "json",
                "presentation_style": "detailed",
                "include_metadata": False,
                "include_scores": False
            },
            "html_cards": {
                "conv_id": "test_html",
                "original_query": "×”×—×œ×˜×•×ª ××—×¨×•× ×•×ª",
                "intent": "search",
                "entities": {},
                "output_format": "html",
                "presentation_style": "compact",
                "include_metadata": True,
                "include_scores": False
            },
            "plain_text_list": {
                "conv_id": "test_plain",
                "original_query": "×ª×§×¦×™×‘ ×—×™× ×•×š",
                "intent": "search",
                "entities": {"topic": "×ª×§×¦×™×‘"},
                "output_format": "plain_text",
                "presentation_style": "list",
                "include_metadata": False,
                "include_scores": False
            },
            "summary_brief": {
                "conv_id": "test_summary",
                "original_query": "×”×—×œ×˜×•×ª ×××©×œ×” 37",
                "intent": "search",
                "entities": {"government_number": 37},
                "output_format": "summary",
                "presentation_style": "compact",
                "include_metadata": False,
                "include_scores": False
            },
            "count_result": {
                "conv_id": "test_count",
                "original_query": "×›××” ×”×—×œ×˜×•×ª ×××©×œ×” 37",
                "intent": "count",
                "entities": {"government_number": 37},
                "output_format": "markdown",
                "presentation_style": "detailed",
                "include_metadata": True,
                "include_scores": False,
                "use_count_result": True
            }
        }
    
    def mock_format_hebrew_date(self, date_str: str) -> str:
        """Mock Hebrew date formatting."""
        if not date_str:
            return "×ª××¨×™×š ×œ× ×–××™×Ÿ"
        
        # Simple mock: 2023-05-15 -> 15 ×‘×××™ 2023
        parts = date_str.split("-")
        if len(parts) >= 3:
            year, month, day = parts[0], parts[1], parts[2]
            month_names = {
                "01": "×™× ×•××¨", "02": "×¤×‘×¨×•××¨", "03": "××¨×¥", "04": "××¤×¨×™×œ",
                "05": "×××™", "06": "×™×•× ×™", "07": "×™×•×œ×™", "08": "××•×’×•×¡×˜",
                "09": "×¡×¤×˜××‘×¨", "10": "××•×§×˜×•×‘×¨", "11": "× ×•×‘××‘×¨", "12": "×“×¦××‘×¨"
            }
            hebrew_month = month_names.get(month, month)
            return f"{int(day)} ×‘{hebrew_month} {year}"
        return date_str
    
    def mock_format_government_info(self, result: Dict) -> str:
        """Mock government information formatting."""
        parts = []
        
        if result.get("government_number"):
            parts.append(f"×××©×œ×” {result['government_number']}")
        
        if result.get("decision_number"):
            parts.append(f"×”×—×œ×˜×” {result['decision_number']}")
        
        if result.get("decision_date"):
            date_str = self.mock_format_hebrew_date(result["decision_date"])
            parts.append(date_str)
        
        return " | ".join(parts) if parts else "××™×“×¢ ×œ× ×–××™×Ÿ"
    
    def mock_format_markdown(self, scenario: Dict) -> str:
        """Mock Markdown formatting."""
        query = scenario["original_query"]
        intent = scenario["intent"]
        style = scenario["presentation_style"]
        include_metadata = scenario["include_metadata"]
        include_scores = scenario["include_scores"]
        
        results = self.sample_results
        if scenario.get("use_count_result"):
            results = [{"count": 42}]
        
        lines = []
        
        # Header
        if intent == "count":
            lines.append(f"# ×ª×•×¦××•×ª ×¡×¤×™×¨×”: {query}")
            if results and "count" in results[0]:
                count = results[0]["count"]
                lines.append(f"\n**××¡×¤×¨ ×”×”×—×œ×˜×•×ª:** {count}")
                return "\n".join(lines)
        else:
            lines.append(f"# ×ª×•×¦××•×ª ×—×™×¤×•×©: {query}")
            lines.append(f"\n**× ××¦××• {len(results)} ×ª×•×¦××•×ª**")
        
        # Evaluation
        if include_metadata:
            lines.append(f"\n**××™×›×•×ª ×”×ª×•×¦××•×ª:** {self.sample_evaluation['overall_score']:.2f} ({self.sample_evaluation['relevance_level']})")
        
        lines.append("\n---\n")
        
        # Results
        for i, result in enumerate(results, 1):
            if style == "detailed":
                lines.append(f"## {i}. {result.get('title', '×œ×œ× ×›×•×ª×¨×ª')}")
                lines.append(f"\n**××™×“×¢ ×›×œ×œ×™:** {self.mock_format_government_info(result)}")
                
                content = result.get('content', '')
                if content:
                    lines.append(f"\n**×ª×•×›×Ÿ:**\n{content}")
                
                if include_scores and "_ranking" in result:
                    score = result["_ranking"]["total_score"]
                    explanation = result["_ranking"]["explanation"]
                    lines.append(f"\n*×¦×™×•×Ÿ: {score:.2f} ({explanation})*")
                
                lines.append("\n---\n")
                
            elif style == "compact":
                title = result.get('title', '×œ×œ× ×›×•×ª×¨×ª')
                gov_info = self.mock_format_government_info(result)
                lines.append(f"**{i}. {title}**")
                lines.append(f"   {gov_info}")
                lines.append("")
        
        return "\n".join(lines)
    
    def mock_format_json(self, scenario: Dict) -> str:
        """Mock JSON formatting."""
        query = scenario["original_query"]
        intent = scenario["intent"]
        include_metadata = scenario["include_metadata"]
        include_scores = scenario["include_scores"]
        
        results = self.sample_results
        if scenario.get("use_count_result"):
            results = [{"count": 42}]
        
        # Clean results for JSON
        clean_results = []
        for result in results:
            clean_result = {
                "id": result.get("id"),
                "title": result.get("title"),
                "content": result.get("content"),
                "government_number": result.get("government_number"),
                "decision_number": result.get("decision_number"),
                "decision_date": result.get("decision_date"),
                "topics": result.get("topics", []),
                "ministries": result.get("ministries", []),
                "status": result.get("status")
            }
            
            if include_scores and "_ranking" in result:
                clean_result["ranking"] = result["_ranking"]
            
            clean_results.append(clean_result)
        
        response_data = {
            "query": query,
            "intent": intent,
            "total_results": len(results),
            "results": clean_results
        }
        
        if include_metadata:
            response_data["metadata"] = {
                "evaluation": self.sample_evaluation,
                "formatted_at": "2023-06-27T12:00:00"
            }
        
        return json.dumps(response_data, ensure_ascii=False, indent=2)
    
    def mock_format_html(self, scenario: Dict) -> str:
        """Mock HTML formatting."""
        query = scenario["original_query"]
        results = self.sample_results
        
        html_parts = [
            "<!DOCTYPE html>",
            "<html dir='rtl' lang='he'>",
            "<head>",
            "    <meta charset='UTF-8'>",
            f"    <title>×ª×•×¦××•×ª ×—×™×¤×•×©: {query}</title>",
            "</head>",
            "<body>",
            f"<h1>×ª×•×¦××•×ª ×—×™×¤×•×©: {query}</h1>",
            f"<p><strong>× ××¦××• {len(results)} ×ª×•×¦××•×ª</strong></p>"
        ]
        
        for i, result in enumerate(results, 1):
            html_parts.append("<div class='result-card'>")
            html_parts.append(f"<div class='result-title'>{i}. {result.get('title', '×œ×œ× ×›×•×ª×¨×ª')}</div>")
            html_parts.append(f"<div class='result-meta'>{self.mock_format_government_info(result)}</div>")
            html_parts.append("</div>")
        
        html_parts.extend(["</body>", "</html>"])
        return "\n".join(html_parts)
    
    def mock_format_plain_text(self, scenario: Dict) -> str:
        """Mock plain text formatting."""
        query = scenario["original_query"]
        results = self.sample_results
        
        lines = [
            f"×ª×•×¦××•×ª ×—×™×¤×•×©: {query}",
            "=" * 50,
            f"× ××¦××• {len(results)} ×ª×•×¦××•×ª",
            "\n" + "-" * 50
        ]
        
        for i, result in enumerate(results, 1):
            lines.append(f"\n{i}. {result.get('title', '×œ×œ× ×›×•×ª×¨×ª')}")
            lines.append(f"   {self.mock_format_government_info(result)}")
            lines.append("")
        
        return "\n".join(lines)
    
    def mock_format_summary(self, scenario: Dict) -> str:
        """Mock summary formatting."""
        query = scenario["original_query"]
        intent = scenario["intent"]
        results = self.sample_results
        
        if intent == "count":
            return f"× ××¦××• 42 ×”×—×œ×˜×•×ª ×¢×‘×•×¨ ×”×©××™×œ×ª× '{query}'"
        
        if not results:
            return f"×œ× × ××¦××• ×ª×•×¦××•×ª ×¢×‘×•×¨ ×”×©××™×œ×ª× '{query}'"
        
        if len(results) == 1:
            result = results[0]
            title = result.get('title', '×”×—×œ×˜×”')
            gov_info = self.mock_format_government_info(result)
            return f"× ××¦××” ×”×—×œ×˜×”: {title} ({gov_info})"
        
        # Multiple results
        first_result = results[0]
        title = first_result.get('title', '')[:60] + "..." if len(first_result.get('title', '')) > 60 else first_result.get('title', '')
        
        return f"× ××¦××• {len(results)} ×”×—×œ×˜×•×ª ×¢×‘×•×¨ '{query}' | ×”×ª×•×¦××” ×”×¨××©×•× ×”: {title} | × ×•×©××™×: ×—×™× ×•×š, ×˜×›× ×•×œ×•×’×™×”"
    
    def mock_format_response(self, scenario: Dict) -> Dict[str, Any]:
        """Mock response formatting based on output format."""
        
        output_format = scenario["output_format"]
        
        if output_format == "markdown":
            formatted_content = self.mock_format_markdown(scenario)
        elif output_format == "json":
            formatted_content = self.mock_format_json(scenario)
        elif output_format == "html":
            formatted_content = self.mock_format_html(scenario)
        elif output_format == "plain_text":
            formatted_content = self.mock_format_plain_text(scenario)
        elif output_format == "summary":
            formatted_content = self.mock_format_summary(scenario)
        else:
            formatted_content = f"Unsupported format: {output_format}"
        
        results_count = 1 if scenario.get("use_count_result") else len(self.sample_results)
        
        return {
            "success": True,
            "conv_id": scenario["conv_id"],
            "formatted_response": formatted_content,
            "format_used": output_format,
            "style_used": scenario["presentation_style"],
            "total_results": results_count,
            "metadata": {
                "formatted_at": "2023-06-27T12:00:00",
                "include_metadata": scenario["include_metadata"],
                "include_scores": scenario["include_scores"]
            }
        }
    
    def test_scenario(self, scenario_name: str, scenario: Dict) -> Dict[str, Any]:
        """Test a single formatting scenario."""
        
        print(f"\nğŸ“‹ Testing: {scenario_name}")
        print(f"ğŸ“¥ Query: '{scenario['original_query']}'")
        print(f"ğŸ¯ Intent: {scenario['intent']}")
        print(f"ğŸ“Š Format: {scenario['output_format']}")
        print(f"ğŸ¨ Style: {scenario['presentation_style']}")
        
        try:
            # Mock formatting
            formatting_result = self.mock_format_response(scenario)
            
            # Validate results
            formatted_content = formatting_result["formatted_response"]
            
            print(f"    âœ… Formatted successfully")
            print(f"    ğŸ“ˆ Format: {formatting_result['format_used']}")
            print(f"    ğŸ¨ Style: {formatting_result['style_used']}")
            print(f"    ğŸ“Š Results: {formatting_result['total_results']}")
            print(f"    ğŸ“ Content length: {len(formatted_content)} characters")
            
            # Display sample content
            sample_lines = formatted_content.split('\n')[:3]  # First 3 lines
            for i, line in enumerate(sample_lines):
                if line.strip():
                    print(f"    ğŸ“ Sample: {line.strip()[:80]}{'...' if len(line.strip()) > 80 else ''}")
                    break
            
            # Validate format-specific expectations
            passed = True
            
            # Common validations
            if scenario["original_query"] not in formatted_content:
                print(f"    âš ï¸ Query '{scenario['original_query']}' not found in output")
                passed = False
            
            # Format-specific validations
            if scenario["output_format"] == "markdown":
                if "#" not in formatted_content:
                    print(f"    âš ï¸ Markdown headers not found")
                    passed = False
                if scenario["include_scores"] and "×¦×™×•×Ÿ:" not in formatted_content:
                    print(f"    âš ï¸ Scores not included as expected")
                    passed = False
                    
            elif scenario["output_format"] == "json":
                try:
                    json_data = json.loads(formatted_content)
                    if "query" not in json_data:
                        print(f"    âš ï¸ JSON missing required 'query' field")
                        passed = False
                except json.JSONDecodeError:
                    print(f"    âŒ Invalid JSON format")
                    passed = False
                    
            elif scenario["output_format"] == "html":
                if "<!DOCTYPE html>" not in formatted_content:
                    print(f"    âš ï¸ HTML doctype not found")
                    passed = False
                if "dir='rtl'" not in formatted_content:
                    print(f"    âš ï¸ RTL direction not set for Hebrew")
                    passed = False
                    
            elif scenario["output_format"] == "summary":
                if len(formatted_content) > 300:
                    print(f"    âš ï¸ Summary too long ({len(formatted_content)} chars)")
                    passed = False
            
            # Intent-specific validations
            if scenario["intent"] == "count":
                if scenario.get("use_count_result") and "42" not in formatted_content:
                    print(f"    âš ï¸ Count result not displayed")
                    passed = False
            
            result = {
                "scenario_name": scenario_name,
                "passed": passed,
                "format": scenario["output_format"],
                "style": scenario["presentation_style"],
                "content_length": len(formatted_content),
                "formatting_result": formatting_result
            }
            
            self.test_results.append(result)
            
            if passed:
                print(f"    ğŸ‰ Scenario '{scenario_name}' passed!")
            else:
                print(f"    âŒ Scenario '{scenario_name}' failed!")
            
            return result
            
        except Exception as e:
            print(f"    âŒ Error in scenario '{scenario_name}': {e}")
            result = {
                "scenario_name": scenario_name,
                "passed": False,
                "error": str(e)
            }
            self.test_results.append(result)
            return result
    
    def run_all_tests(self):
        """Run all formatting tests."""
        print("ğŸš€ Starting Formatter Bot Functionality Tests")
        print("=" * 60)
        
        # Run all test scenarios
        for scenario_name, scenario in self.test_scenarios.items():
            self.test_scenario(scenario_name, scenario)
        
        # Summary
        print("\n" + "=" * 60)
        print("ğŸ“Š Test Summary")
        
        passed = sum(1 for result in self.test_results if result.get("passed", False))
        total = len(self.test_results)
        
        print(f"âœ… Passed: {passed}/{total} scenarios")
        print(f"âŒ Failed: {total - passed}/{total} scenarios")
        
        if passed == total:
            print("ğŸ‰ All formatting tests passed!")
            print("âœ… Response formatting is working correctly")
        else:
            print(f"âš ï¸ {total - passed} tests failed")
            
            # Show failed tests
            failed_tests = [r for r in self.test_results if not r.get("passed", False)]
            for failed in failed_tests:
                error = failed.get("error", "Test failure")
                print(f"    âŒ {failed['scenario_name']}: {error}")
        
        # Format analysis
        print("\nğŸ¨ Format Analysis:")
        format_stats = {}
        for result in self.test_results:
            if "format" in result:
                format_name = result["format"]
                if format_name not in format_stats:
                    format_stats[format_name] = {"passed": 0, "total": 0, "avg_length": 0}
                
                format_stats[format_name]["total"] += 1
                if result.get("passed", False):
                    format_stats[format_name]["passed"] += 1
                
                if "content_length" in result:
                    format_stats[format_name]["avg_length"] += result["content_length"]
        
        for format_name, stats in format_stats.items():
            avg_length = stats["avg_length"] / stats["total"] if stats["total"] > 0 else 0
            success_rate = stats["passed"] / stats["total"] * 100 if stats["total"] > 0 else 0
            print(f"    ğŸ“ˆ {format_name}: {success_rate:.0f}% success, avg length: {avg_length:.0f} chars")
        
        # Content length distribution
        if self.test_results:
            lengths = [r.get("content_length", 0) for r in self.test_results if "content_length" in r]
            if lengths:
                print(f"\nğŸ“ Content Length Distribution:")
                print(f"    ğŸ“Š Shortest: {min(lengths)} chars")
                print(f"    ğŸ“Š Average: {sum(lengths) / len(lengths):.0f} chars")
                print(f"    ğŸ“Š Longest: {max(lengths)} chars")
        
        return passed == total

def main():
    """Main test runner."""
    test_runner = SimpleFormatterTest()
    success = test_runner.run_all_tests()
    return success

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)